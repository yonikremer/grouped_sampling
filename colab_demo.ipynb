{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view-in-github",
    "colab_type": "text"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/yonikremer/grouped_sampling/blob/master/colab_demo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "u3Z6Y8zjvMF3"
   },
   "source": [
    "# Use grouped sampling:"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "!pip install -q transformers grouped_sampling torch beautifulsoup4 accelerate sentencepiece"
   ],
   "metadata": {
    "id": "YLjV8dQT845B",
    "outputId": "04c7f289-d20b-4834-d4fa-284a347c1fb8",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "execution_count": 1,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.8/6.8 MB\u001b[0m \u001b[31m26.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
      "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
      "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m212.8/212.8 KB\u001b[0m \u001b[31m23.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m67.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.8/199.8 KB\u001b[0m \u001b[31m17.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m74.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Building wheel for grouped_sampling (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NQtDCVYAvMF5",
    "outputId": "1cfeae7f-02ee-493d-b0c7-de68141589c3",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/huggingface_hub/utils/_deprecation.py:229: FutureWarning: 'list_models' currently returns a list of objects but is planned to be a generator starting from version 0.14 in order to implement pagination. Please avoid to use `list_models(...).__len__` or explicitly convert the output to a list first with `list(iter(list_models)(...))`.\n",
      "  warnings.warn(self._deprecation_msg.format(attr_name=attr_name), FutureWarning)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Your prompt:\n",
      "I had so much fun I the\n",
      "Text generated by Non grouped sampling in 120.7679536579999 seconds:\n",
      " last few days.\n",
      "I had a great time with my friends and family, I got to see the movie \"The Hunger Games\" which was awesome!\n",
      "It's so good that it made me cry at one point in the film.\n",
      "My\n"
     ]
    }
   ],
   "source": [
    "import timeit\n",
    "from math import ceil, floor\n",
    "\n",
    "from transformers import AutoConfig\n",
    "from grouped_sampling import GroupedSamplingPipeLine\n",
    "\n",
    "\n",
    "def compare_generators(\n",
    "    pipeline: GroupedSamplingPipeLine,\n",
    "    prompt: str,\n",
    "    num_tokens: int,\n",
    "):\n",
    "    \"\"\"Compares grouped and non-grouped text generators\"\"\"\n",
    "    print(f\"Your prompt:\")\n",
    "    print(prompt)\n",
    "\n",
    "    start_non_grouped = timeit.default_timer()\n",
    "    non_grouped_ans: str = pipeline(\n",
    "        prompt_s=prompt, max_new_tokens=num_tokens, return_full_text=False\n",
    "    )[\"generated_text\"]\n",
    "    stop_non_grouped = timeit.default_timer()\n",
    "    non_grouped_time = stop_non_grouped - start_non_grouped\n",
    "    print(f\"Text generated by Non grouped sampling\" f\" in {non_grouped_time} seconds:\")\n",
    "    print(non_grouped_ans)\n",
    "\n",
    "    pipeline.wrapped_model.group_size = 1024\n",
    "    grouped_generator = pipeline\n",
    "    start_grouped_generation = timeit.default_timer()\n",
    "    grouped_ans: str = grouped_generator(\n",
    "        prompt_s=prompt, max_new_tokens=num_tokens, return_full_text=False\n",
    "    )[\"generated_text\"]\n",
    "    stop_grouped_generation = timeit.default_timer()\n",
    "    grouped_time = stop_grouped_generation - start_grouped_generation\n",
    "    print(f\"Text generated by grouped sampling\" f\" in {grouped_time} seconds:\")\n",
    "    print(grouped_ans)\n",
    "\n",
    "\n",
    "model_name = \"facebook/opt-iml-1.3b\"\n",
    "prompt = \"I had so much fun I the\"  # @param {type:\"string\"}\n",
    "num_tokens = 50  # @param {type:\"integer\", min:1}\n",
    "top_p = 1  # @param {type:\"slider\", min:0.0, max:1.0, step:0.05}\n",
    "top_k = None  # @param {type:\"integer\"}\n",
    "temperature = 1  # @param {type:\"number\", min:0.000000001}\n",
    "\n",
    "non_grouped_generator = GroupedSamplingPipeLine(\n",
    "    model_name=model_name,\n",
    "    group_size=1,\n",
    "    temp=temperature,\n",
    "    top_k=top_k,\n",
    "    top_p=top_p,\n",
    "    end_of_sentence_stop=False,\n",
    "    load_in_8bit=False,\n",
    ")\n",
    "\n",
    "compare_generators(non_grouped_generator, prompt, num_tokens)\n",
    "del non_grouped_generator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "aPUXtswQvMF9"
   },
   "source": [
    "# Change the hyper-parameters to see what will happen!"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "final_project_demo.ipynb",
   "provenance": [],
   "include_colab_link": true
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3.9.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "4c667eb41e29edd16195f2712b5916a29b36d4e6f17fe7c0a7edbb7a73b8ba8a"
   }
  },
  "accelerator": "GPU"
 },
 "nbformat": 4,
 "nbformat_minor": 0
}