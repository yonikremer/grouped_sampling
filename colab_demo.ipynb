{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yonikremer/grouped_sampling/blob/master/colab_demo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "u3Z6Y8zjvMF3"
      },
      "source": [
        "# Use grouped sampling:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NQtDCVYAvMF5"
      },
      "outputs": [],
      "source": [
        "!pip install -q transformers grouped_sampling\n",
        "\n",
        "import timeit, sys, os\n",
        "from math import ceil, floor\n",
        "\n",
        "from transformers import AutoConfig\n",
        "from grouped_sampling import SamplingGenerator\n",
        "\n",
        "\n",
        "def compare_generators(\n",
        "        non_grouped_generator: SamplingGenerator,\n",
        "        prompt: str,\n",
        "        num_tokens: int,\n",
        "        group_size: int\n",
        "        ):\n",
        "    \"\"\"Compares grouped and non-grouped text generators\"\"\"\n",
        "    print(f\"Your prompt:\")\n",
        "    print(prompt)\n",
        "\n",
        "    start_non_grouped = timeit.default_timer()\n",
        "    non_grouped_ans: str = non_grouped_generator(\n",
        "        prompt_s=prompt,\n",
        "        max_new_tokens=num_tokens,\n",
        "        return_full_text=False\n",
        "    )[\"generated_text\"]\n",
        "    stop_non_grouped = timeit.default_timer()\n",
        "    non_grouped_time = stop_non_grouped - start_non_grouped\n",
        "    print(f\"Text generated by Non grouped sampling\"\n",
        "          f\" in {non_grouped_time} seconds:\")\n",
        "    print(non_grouped_ans)\n",
        "\n",
        "    non_grouped_generator.group_size = group_size\n",
        "    grouped_generator = non_grouped_generator\n",
        "    start_grouped_generation = timeit.default_timer()\n",
        "    grouped_ans: str = grouped_generator(\n",
        "        prompt_s=prompt,\n",
        "        max_new_tokens=num_tokens,\n",
        "        return_full_text=False\n",
        "    )[\"generated_text\"]\n",
        "    stop_grouped_generation = timeit.default_timer()\n",
        "    grouped_time = stop_grouped_generation - start_grouped_generation\n",
        "    print(f\"Text generated by grouped sampling\"\n",
        "          f\" in {grouped_time} seconds:\")\n",
        "    print(grouped_ans)\n",
        "\n",
        "\n",
        "model_name = \"gpt2-medium\" #@param [\"facebook/opt-125m\", \"facebook/opt-350m\", \"gpt2\", \"gpt2-medium\"]\n",
        "prompt = \"\" #@param {type:\"string\"}\n",
        "num_tokens = 100 #@param {type:\"integer\", min:1}\n",
        "top_p = 1 #@param {type:\"slider\", min:0.0, max:1.0, step:0.05}\n",
        "top_k = 100000 #@param {type:\"integer\"}\n",
        "group_size = 100 #@param {type:\"integer\"}\n",
        "temperature = 1 #@param {type:\"number\", min:0.000000001}\n",
        "\n",
        "vocab_size = AutoConfig.from_pretrained(model_name).vocab_size\n",
        "\n",
        "num_groups = ceil(num_tokens / group_size)\n",
        "actual_top_k = min(top_k, floor(vocab_size * top_p))\n",
        "max_num_calls = num_groups\n",
        "previous_methods_max_num_calls = num_tokens\n",
        "if top_p == 1.0:\n",
        "    top_p = None\n",
        "elif top_p == 0.0:\n",
        "    top_k = 1\n",
        "    top_p = None\n",
        "elif top_k >= vocab_size:\n",
        "    top_k = None\n",
        "else:\n",
        "    raise ValueError(\"When using sampling from distribution, You must use either top k or top k and no both\")\n",
        "non_grouped_generator = SamplingGenerator(\n",
        "    model_name=model_name,\n",
        "    group_size=group_size,\n",
        "    temp=temperature, \n",
        "    top_k=top_k, \n",
        "    top_p=top_p,\n",
        "    end_of_sentence_stop=False,\n",
        "    )\n",
        "\n",
        "print(f\"The model will generate {num_tokens} tokens (words or parts of words)\")\n",
        "print(f\"It will call the model at most {max_num_calls} times to the model\")\n",
        "print(f\"Previous methods will need up to {previous_methods_max_num_calls} call to the model to generate the same text\")\n",
        "\n",
        "compare_generators(non_grouped_generator, prompt, num_tokens, group_size)\n",
        "del non_grouped_generator"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "aPUXtswQvMF9"
      },
      "source": [
        "# Change the hyper-parameters to see what will happen!"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "final_project_demo.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3.9.12",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.12"
    },
    "vscode": {
      "interpreter": {
        "hash": "4c667eb41e29edd16195f2712b5916a29b36d4e6f17fe7c0a7edbb7a73b8ba8a"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}