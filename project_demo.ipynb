{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Demo - this is a demo for the project - \n",
    "\n",
    "Here yu can try out the sampling method I intreduced in my research.\n",
    "I highly reccomend to lookup [my research project](https://github.com/yonikremer/final_project) for more information."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Set Up"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%pip install -q transformers;\n",
    "%pip install -q torch;"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import timeit\n",
    "from math import ceil"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from transformers import AutoConfig, BatchEncoding\n",
    "from torch import cuda, device"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from text_generator import TextGenerator"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "if not cuda.is_available():\n",
    "    print(\"Warning: CUDA not available\")\n",
    "    print(\"Running on CPU wil be very slow\")\n",
    "    print(\"it's highly recommended to use colab's GPU runtime\")\n",
    "    exit()\n",
    "else:\n",
    "    cuda = device('cuda')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Important Notes:\n",
    "\n",
    "\n",
    "*   Please make sure the model you choose is not too big for your hardware\n",
    "*   gpt2 is the smallest version of gpt-2, gpt2-xl is the largest\n",
    "*   bloom is the largest vesion of bloom\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def compare_generators(grouped_generator: TextGenerator, non_grouped_generator: TextGenerator, prompt: str, num_tokens: int):\n",
    "    \"\"\"Compares grouped and non-grouped text generators\"\"\"\n",
    "    print(f\"Your prompt:\")\n",
    "    print(prompt)\n",
    "\n",
    "    start_non_grouped = timeit.default_timer()\n",
    "    non_grouped_ans: str = non_grouped_generator(prompt, num_tokens)\n",
    "    stop_non_grouped = timeit.default_timer()\n",
    "    non_grouped_time = stop_non_grouped - start_non_grouped\n",
    "    print(f\"Text generated by Non grouped sampling in {non_grouped_time} seconds:\")\n",
    "    print(non_grouped_ans)\n",
    "\n",
    "    start_grouped_generation = timeit.default_timer()\n",
    "    grouped_ans: str = grouped_generator(prompt, num_tokens)\n",
    "    stop_grouped_generation = timeit.default_timer()\n",
    "    grouped_time = stop_grouped_generation - start_grouped_generation\n",
    "    print(f\"Text generated by grouped sampling in {grouped_time} seconds:\")\n",
    "    print(grouped_ans)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Use grouped sampling:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model_name = \"bigscience/bloom-1b1\" #@param [\"facebook/opt-125m\", \"facebook/opt-1.3b\", \"facebook/opt-350m\", \"gpt2\", \"gpt2-medium\", \"gpt2-large\", \"bigscience/bloom-1b1\", \"bigscience/bloom-560m\"]\n",
    "prompt = \"I was so sad to graduate high school that I almost \" #@param {type:\"string\"}\n",
    "num_tokens = 4 #@param {type:\"integer\", min:1}\n",
    "top_p = 0.4 #@param {type:\"slider\", min:0.0, max:1.0, step:0.05}\n",
    "top_k = 10000000000000000000000 #@param {type:\"integer\"}\n",
    "group_size = 2 #@param {type:\"integer\"}\n",
    "temperature = 1 #@param {type:\"number\", min:0.000000001}\n",
    "sampling_type = 'from distribution' #@param [\"from distribution\", \"as tree\"]\n",
    "\n",
    "\n",
    "vocab_size = AutoConfig.from_pretrained(model_name).vocab_size\n",
    "\n",
    "num_groups = ceil(num_tokens / group_size)\n",
    "actual_top_k = min(top_k, vocab_size * top_p)\n",
    "\n",
    "if sampling_type == \"as tree\":\n",
    "    max_num_calls = sum((actual_top_k ** i) for i in range(num_groups))    \n",
    "    previous_methods_max_num_calls = sum((actual_top_k ** i) for i in range(num_tokens))\n",
    "    grouped_generator = TreeGen(model_name, group_size, top_k, top_p, temperature)\n",
    "    non_grouped_generator = TreeGen(model_name, 1, top_k, top_p, temperature)\n",
    "else:\n",
    "    max_num_calls = num_groups\n",
    "    previous_methods_max_num_calls = num_tokens\n",
    "    if top_p == 1.0:\n",
    "        top_p = None\n",
    "    elif top_p == 0.0:\n",
    "        top_k = 1\n",
    "        top_p = None\n",
    "    elif top_k >= vocab_size:\n",
    "        top_k = None\n",
    "    else:\n",
    "        raise ValueError(\"When using sampling from distribution, You must use either top k or top k and no both\")\n",
    "    grouped_generator = SampleGen(model_name, group_size, top_k, top_p, temperature)\n",
    "    non_grouped_generator = SampleGen(model_name, 1, top_k, top_p, temperature)\n",
    "\n",
    "print(f\"The model will generate {num_tokens} tokens (words or parts of words)\")\n",
    "print(f\"It will call the model at most {max_num_calls} times to the model\")\n",
    "print(f\"Previous methods will need up to {previous_methods_max_num_calls} call to the model to generate the same text\")\n",
    "\n",
    "compare_generators(grouped_generator, non_grouped_generator, prompt, num_tokens)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Change the hyper-parameters to see what will happen!"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "global_tokenizer = non_grouped_generator.tokenizer\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "tokenized_prompt_ten = global_tokenizer(prompt, return_tensors=\"pt\")\n",
    "if isinstance(tokenized_prompt_ten, dict) or isinstance(tokenized_prompt_ten, BatchEncoding):\n",
    "    if \"input_ids\" in tokenized_prompt_ten.keys():\n",
    "        tokenized_prompt_ten = tokenized_prompt_ten[\"input_ids\"]\n",
    "tokenized_prompt_ten.shape[1]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "colab": {
   "name": "final_project_demo.ipynb",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3.9.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "4c667eb41e29edd16195f2712b5916a29b36d4e6f17fe7c0a7edbb7a73b8ba8a"
   }
  },
  "accelerator": "GPU"
 },
 "nbformat": 4,
 "nbformat_minor": 0
}