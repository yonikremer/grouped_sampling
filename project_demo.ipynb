{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yonikremer/final_project/blob/master/project_demo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JHhVA_vxD6NV"
      },
      "source": [
        "# Demo - this is a demo for the project - \n",
        "\n",
        "Here yu can try out the sampling method I intreduced in my research.\n",
        "I highly reccomend to lookup [my research project](https://github.com/yonikremer/final_project) for more information."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yDbjm_yg5-5t"
      },
      "source": [
        "# Set Up"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0A1EbQSf6WtO"
      },
      "source": [
        "Please run the following cell once, than restart the kernal and run the other cells by order"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "th_b4EHG4b4l"
      },
      "outputs": [],
      "source": [
        "%pip install -q transformers;\n",
        "%pip install -q torch;"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import List, Tuple, Iterable, Set, Dict\n",
        "from functools import reduce\n",
        "import random\n",
        "from copy import deepcopy"
      ],
      "metadata": {
        "id": "NTkbZjWS5pSc"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Rou3pmrewyXG",
        "outputId": "78cdf10c-9d88-4e3e-b148-ccf81ea2983d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-af16a47c9b9f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mcopy\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdeepcopy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAutoTokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAutoModelForCausalLM\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAutoConfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPreTrainedModel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcuda\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'transformers'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM, AutoConfig, PreTrainedModel\n",
        "\n",
        "from torch import cuda, device\n",
        "from torch.nn import Softmax"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yNv-zShOcIPB"
      },
      "outputs": [],
      "source": [
        "if not cuda.is_available():\n",
        "    print(\"Warning: CUDA not available\")\n",
        "    print(\"Running on CPU wil be very slow\")\n",
        "    print(\"it's highly recommended to use colab's GPU runtime\")\n",
        "    exit()\n",
        "else:\n",
        "    cuda = device('cuda')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dNzB89UE5anV"
      },
      "source": [
        "# Important Notes:\n",
        "\n",
        "\n",
        "*   Please make sure the model you choose is not too big for your hardware\n",
        "*   gpt2 is the smallest version of gpt-2, gpt2-xl is the largest\n",
        "*   bloom is the largest vesion of bloom\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lhKi5OED6J30"
      },
      "source": [
        "# Selecting a Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "lDyw05iOoo98"
      },
      "outputs": [],
      "source": [
        "#@title Select a model from the list\n",
        "\n",
        "model_name = \"gpt2\" #@param [\"facebook/opt-125m\", \"facebook/opt-1.3b\", \"facebook/opt-13b\", \"facebook/opt-125m\", \"facebook/opt-2.7b\", \"facebook/opt-30b\", \"facebook/opt-350m\", \"facebook/opt-6.7b\", \"facebook/opt-66b\", \"gpt2\", \"gpt2-medium\", \"gpt2-xl\", \"gpt2-large\", \"bigscience/bloom\", \"bigscience/bloom-7b1\", \"bigscience/bloom-6b3\", \"bigscience/bloom-3b\", \"bigscience/bloom-1b7\", \"bigscience/bloom-1b1\", \"bigscience/bloom-560m\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oFpZRiSR9mlh"
      },
      "outputs": [],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(model_name);\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name).cuda();\n",
        "vocab_size = AutoConfig.from_pretrained(model_name).vocab_size;"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def print_bases(obj):\n",
        "    curr_base = type(obj)\n",
        "    while curr_base != object:\n",
        "        print(curr_base)\n",
        "        curr_base = curr_base.__base__\n",
        "\n",
        "print_bases(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e9VAuvvtCA29",
        "outputId": "7c280f9d-51fe-4ddf-97a7-a0608d2921fb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'transformers.models.gpt2.modeling_gpt2.GPT2LMHeadModel'>\n",
            "<class 'transformers.models.gpt2.modeling_gpt2.GPT2PreTrainedModel'>\n",
            "<class 'transformers.modeling_utils.PreTrainedModel'>\n",
            "<class 'torch.nn.modules.module.Module'>\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer(\"Tokenize this\", return_tensors=\"pt\")"
      ],
      "metadata": {
        "id": "R85qVazLFJ9X",
        "outputId": "a3c7d328-a8db-4edb-cd9a-96dfa20ee7f5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input_ids': tensor([[30642,  1096,   428]]), 'attention_mask': tensor([[1, 1, 1]])}"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7S5lrpOO0jS8"
      },
      "source": [
        "# The code I use to sample:"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## distribution sampling  "
      ],
      "metadata": {
        "id": "qHgtzS6dvM3Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def grouped_top_p_sampling(prob_mat: List[List[float]], top_p: float, org_used_tokens: List[int] = list()):\n",
        "    used_tokens = deepcopy(org_used_tokens)\n",
        "    answer = []\n",
        "    for curr_token_prob_list in prob_mat:\n",
        "        for used_token in used_tokens:\n",
        "            curr_token_prob_list[used_token] = 0.0\n",
        "\n",
        "        indexed_prob: Dict[int, float] = {i: prob for i, prob in enumerate(curr_token_prob_list)}  # O(vocab_size)\n",
        "        sorted_items = sorted(indexed_prob.items(), key = lambda item: item[1], reverse=True)\n",
        "        sorted_indexed_prob = {key: value for key, value in sorted_items}\n",
        "        top_p_indexed_prob: Dict[int, float] = {}\n",
        "        prob_sum: float = 0.0\n",
        "        for i, (curr_token, curr_prob) in enumerate(sorted_indexed_prob.items()):\n",
        "            if i > 0 and prob_sum + curr_prob > top_p:\n",
        "                break\n",
        "            prob_sum += curr_prob\n",
        "            top_p_indexed_prob[curr_token] = curr_prob\n",
        "\n",
        "        weighted_probs = {key: value / prob_sum for key, value in top_p_indexed_prob.items()}\n",
        "\n",
        "        sampled_token: int = random.choices(list(weighted_probs.keys()), weights = weighted_probs.values(), k=1)[0]\n",
        "        answer.append(sampled_token)\n",
        "        used_tokens.append(sampled_token)\n",
        "    return answer\n",
        "\n",
        "def grouped_top_k_sampling(prob_mat: List[List[float]], top_k: int, org_used_tokens: List[int] = list()):\n",
        "    used_tokens = deepcopy(org_used_tokens)\n",
        "    answer = []\n",
        "    for curr_token_prob_list in prob_mat:\n",
        "        for used_token in used_tokens:\n",
        "            curr_token_prob_list[used_token] = 0.0\n",
        "\n",
        "        indexed_prob: Dict[int, float] = {i: prob for i, prob in enumerate(curr_token_prob_list)}  # O(vocab_size)\n",
        "        sorted_items = sorted(indexed_prob.items(), key = lambda item: item[1], reverse=True)\n",
        "        sorted_indexed_prob = {key: value for key, value in sorted_items}\n",
        "        sorted_top_k_keys = list(sorted_indexed_prob.keys())[:top_k]\n",
        "        top_k_indexed_prob: Dict[int, float] = {key: sorted_indexed_prob[key] for key in sorted_top_k_keys}\n",
        "        prob_sum: float = sum(top_k_indexed_prob.values())\n",
        "        weighted_probs = {key: value / prob_sum for key, value in top_k_indexed_prob.items()}\n",
        "\n",
        "        sampled_token: int = random.choices(list(weighted_probs.keys()), weights = weighted_probs.values(), k=1)[0]\n",
        "        answer.append(sampled_token)\n",
        "        used_tokens.append(sampled_token)\n",
        "    return answer\n",
        "\n",
        "def generate_distribution():\n",
        "    pass"
      ],
      "metadata": {
        "id": "YKJqvrxovUYa"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prob_mat = [[random.uniform(0.0, 0.5) for _ in range(10_000)] for __ in range(10)]\n",
        "\n",
        "grouped_top_p_sampling(prob_mat, 0.7)"
      ],
      "metadata": {
        "id": "UjpWpkq-46fr",
        "outputId": "acf09c75-1f57-44ae-a4d0-30744f6e356e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[2997, 3788, 1704, 2024, 7947, 1253, 4888, 4213, 8366, 8302]"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Brute force sampling"
      ],
      "metadata": {
        "id": "xcp_Xnw3vJ8M"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7xP7MjctCfgq"
      },
      "outputs": [],
      "source": [
        "def combinations(mat):\n",
        "    \"\"\"Returns all the lists such that list[j] is in mat[j]\n",
        "    complexity: prod([len(mat[i]) for i in range(len(mat))])\"\"\"\n",
        "    if len(mat) == 1:\n",
        "        return [[mat[0][i]] for i in range(len(mat[0]))]\n",
        "    res = []\n",
        "    for i in mat[0]:\n",
        "        for j in combinations(mat[1:]):\n",
        "            res.append([i] + j)\n",
        "    filtered_res = list(filter(doesnt_have_duplicates, res))\n",
        "    return filtered_res\n",
        "\n",
        "\n",
        "def doesnt_have_duplicates(my_list):\n",
        "    \"\"\"Return if there isn't a repetition in the list\n",
        "    complexity: O(n) where n is the length of the list\"\"\"\n",
        "    return len(my_list) == len(set(my_list))\n",
        "\n",
        "\n",
        "def seq_prob(tokens, prob_mat, org_prompt_prob):\n",
        "    \"\"\"Given the probability matrix and a list of tokens\n",
        "    returns the probability of the sequence\n",
        "    prob_mat[a][b] is the probability of the token with id b the a-th token in the sequence\"\"\"\n",
        "    probability = org_prompt_prob\n",
        "    sequence_length = len(tokens)\n",
        "    for i in range(sequence_length):\n",
        "        curr_token = tokens[i]\n",
        "        probability *= prob_mat[i][curr_token]\n",
        "    return probability\n",
        "\n",
        "\n",
        "def grouped_sampling(prob_mat: List[List[float]], top_p, top_k) -> List[List[int]]:\n",
        "    \"\"\"given a matrix of probabilities, returns a list of lists of tokens\n",
        "    the matrixs is of size group_size x vocab_size\n",
        "    where matrix[i, j] is the probability of token j the i-th token in the group\n",
        "    samples the tokens such that for each place in the group,\n",
        "    at most top_k tokens are sampled and at least one token is sampled\n",
        "    and the added probability of all the tokens is less than or equal top_p\n",
        "    returns a list of where every item is a tuple of a sequense and probability\n",
        "    over all complexity of the function in O(group_size * vocab_size * log(vocab_size))\"\"\"\n",
        "    \n",
        "    # prob_tensor.shape is now (group_size, vocab_size)\n",
        "    posible_tokens = []\n",
        "    already_predicted = set()\n",
        "    for token_prob in prob_mat: # group_size times\n",
        "        vocab_size = len(token_prob)  # O(1)\n",
        "        indexed_prob = list(zip(token_prob, range(vocab_size)))  # O(vocab_size)\n",
        "        sorted_indexed_prob = sorted(indexed_prob, key=lambda x: x[0], reverse=True) # O(vocab_size*log(vocab_size))\n",
        "        curr_k = 0\n",
        "        total_prob = 0\n",
        "        curr_indices = []\n",
        "        for prob, token in sorted_indexed_prob:  # O(top_k)\n",
        "            if total_prob + prob > top_p or curr_k == top_k:\n",
        "                break\n",
        "            if not token in already_predicted:\n",
        "                already_predicted.add(token)\n",
        "                curr_k += 1\n",
        "                total_prob += prob\n",
        "                curr_indices.append(token)\n",
        "        posible_tokens.append(curr_indices)  # O(1)\n",
        "    new_sequences: List[List[int]] = combinations(posible_tokens)  # theta(prod(len(indices[i]) for i in range(group_size)))\n",
        "    # len(indices[i]) < min(top_k, vocab_size)\n",
        "    # therefore the complexity is O(min(top_k, vocab_size) * group_size)\n",
        "    return new_sequences\n",
        "\n",
        "\n",
        "def get_prob_mat(model_name, prompt, group_size):\n",
        "    \"\"\"Returns the probability matrix as a list of lists of floats\"\"\"\n",
        "    if not isinstance(model, PreTrainedModel): \n",
        "        raise NotImplementedError(\"Only AutoModelForCausalLM is supported\")\n",
        "\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
        "    inputs = {name: tensor.cuda() for name, tensor in inputs.items()}\n",
        "\n",
        "    try:\n",
        "        logits_pt_tensor = model(**inputs, labels=inputs[\"input_ids\"]).logits.squeeze(0)\n",
        "    except Exception:\n",
        "        logits_pt_tensor = model(**inputs).logits.squeeze(0)\n",
        "    prob_tensor = Softmax(dim=1)(logits_pt_tensor)\n",
        "    if group_size <= prob_tensor.shape[0]:\n",
        "        prob_tensor = prob_tensor[:group_size, :]\n",
        "        prob_mat = [prob_tensor[i, :].tolist() for i in range(group_size)]\n",
        "    else:\n",
        "        print(\"Warning: the group size is bigger than the length of the model's output\")\n",
        "        print(\"If the length of the model input (in tokens) is n, n will be length of the model's output\")\n",
        "        print(f\"the predicted text will be {group_size - prob_tensor.shape[0]} tokens shorter\")\n",
        "        prob_mat = [prob_tensor[i, :].tolist() for i in range(prob_tensor.shape[0])]\n",
        "      \n",
        "    return prob_mat\n",
        "\n",
        "\n",
        "def flatten(l: list) -> List:\n",
        "    \"\"\"Gets a list where some of the elements might be lists\n",
        "    and adds every item in the inner list to the outer list.\n",
        "    example: [1, [2, 3], 4, [[5]]] -> [1, 2, 3, 4, 5]\n",
        "    Complexity: O(len(the flatten list) + the number of diffrent lists))\"\"\"\n",
        "    new_list = []\n",
        "    for item in l:\n",
        "        if isinstance(item, list):\n",
        "            new_list.extend(flatten(item))\n",
        "        else:\n",
        "            new_list.append(item)\n",
        "    return new_list\n",
        "\n",
        "\n",
        "def complete(model_name, org_prompt, top_p, top_k, num_groups, group_size, org_prompt_prob = 1.0) -> Dict[Tuple[int], float]:\n",
        "    \"\"\"preprocess the prompt and completes the text\n",
        "    model name: str\n",
        "    org_prompt: str\n",
        "    top_p: float (0.0 - 1.0)\n",
        "    top_k: int > 0\n",
        "    num_groups: int > 0\n",
        "    group_size: int > 0\n",
        "    org_prompt_prob: float <= 1\n",
        "    \"\"\"\n",
        "    if isinstance(org_prompt, str):\n",
        "        str_prompt = org_prompt\n",
        "        try:\n",
        "            tokenized_prompt_ten = tokenizer(str_prompt, return_tensors=\"pt\")[\"input_ids\"]\n",
        "        except Exception:\n",
        "            tokenized_prompt_ten = tokenizer(str_prompt, return_tensors=\"pt\")\n",
        "        tokenized_prompt_list = tokenized_prompt_ten.tolist()\n",
        "    elif isinstance(org_prompt, list) or isinstance(org_prompt, tuple):\n",
        "        tokenized_prompt_list = flatten(org_prompt)\n",
        "        str_prompt = tokenizer.decode(tokenized_prompt_list)\n",
        "    else:\n",
        "        print(org_prompt)\n",
        "        raise ValueError(\"org_prompt must be a string or list of integers\")\n",
        "    \n",
        "    prob_mat = get_prob_mat(model_name, str_prompt, group_size)\n",
        "    tokenized_ans_list = grouped_sampling(prob_mat, top_p, top_k)\n",
        "    prob_list: List[Tuple[List[int], float]] = [seq_prob(seq, prob_mat, org_prompt_prob) for seq in tokenized_ans_list]\n",
        "    new_prompts: List[List[int]] = [flatten(tokenized_prompt_list + ans) for ans in tokenized_ans_list]\n",
        "    complition_prob_dict = remove_duplicates(new_prompts, prob_list)\n",
        "    if num_groups == 1:\n",
        "        return complition_prob_dict\n",
        "    new_complitions: Dict[Tuple[int], float] = dict()\n",
        "    for curr_new_prompt, curr_new_prompt_prob in complition_prob_dict.items():\n",
        "        curr_completions: Dict[Tuple[int], float] = complete(model_name, curr_new_prompt, top_p, top_k, num_groups - 1, group_size, curr_new_prompt_prob)\n",
        "        tokens: Tuple[int]\n",
        "        prob: float\n",
        "        for tokens, prob in curr_completions.items():\n",
        "            new_complitions[tokens] = prob\n",
        "    return new_complitions\n",
        "\n",
        "\n",
        "def remove_duplicates(completions: List[List[int]], probs: List[float]) -> Dict[Tuple[int], float]:\n",
        "    \"\"\"Given a list of tokenized answers and the probability of each complition,\n",
        "    removes every repeated completion and every complition that have repeated tokens\"\"\"\n",
        "    filtered_completions: Dict[Tuple[int], float] = dict()\n",
        "    for curr_comp, curr_prob in zip(completions, probs):\n",
        "        try:\n",
        "            cond = len(curr_comp) == len(set(curr_comp))\n",
        "        except TypeError as e:\n",
        "            print(curr_comp)\n",
        "            raise e\n",
        "        if cond:\n",
        "            curr_comp_tuple = tuple(curr_comp)\n",
        "            filtered_completions[curr_comp_tuple] = curr_prob\n",
        "    return filtered_completions\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pDOGorUX0pFg"
      },
      "source": [
        "# Use grouped sampling:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LS3o1Umjx6b3"
      },
      "source": [
        "## How to use previous methods:\n",
        "\n",
        "| brute force         | nucleus (top p)                 | top k                              | greedy         |\n",
        "|---------------------|---------------------------------|------------------------------------|----------------|\n",
        "| top p=0             | 0 < top p < 1                   | top p = 1                          | top p = 0      |\n",
        "| top k >= vocab_size | top k >= the model's vocab size | 1 < top k < the model's vocab size | top k = 1      |\n",
        "| group_size = 1      | group_size = 1                  | group_size = 1                     | group_size = 1 |"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9BUwTv4reNg_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d45b51f5-f15f-45c9-f29d-29d3a7f6252e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The model will generate 16 tokens (words or parts of words)\n",
            "It will call the model at most 21 times to the model\n",
            "Previous methods will need up to 34492631578947368421 call to the model to genrate the same text\n"
          ]
        }
      ],
      "source": [
        ", maximum\n",
        "prompt = \"Hello, I'm am conscious and\" #@param {type:\"string\"}\n",
        "top_p = 0.7 #@param {type:\"slider\", min:0.05, max:1.0, step:0.05}\n",
        "top_k = 20 #@param {type:\"integer\"}\n",
        "group_size = 8 #@param {type:\"integer\"}\n",
        "num_groups = 2 #@param {type:\"integer\"}\n",
        "sampling_type = 'from distribution' #@param [\"from distribution\", \"brute force\"]\n",
        "\n",
        "\n",
        "actual_top_k = min(top_k, vocab_size)\n",
        "num_tokens_genrated = num_groups * group_size\n",
        "\n",
        "if sampling_type == \"brute force\":\n",
        "    max_num_calls = sum((actual_top_k ** i) for i in range(num_groups))    \n",
        "    pervious_methods_max_num_calls = sum((actual_top_k ** i) for i in range(num_tokens_genrated))\n",
        "else:\n",
        "    max_num_calls = num_groups\n",
        "    pervious_methods_max_num_calls = num_tokens_genrated\n",
        "\n",
        "print(f\"The model will generate {num_tokens_genrated} tokens (words or parts of words)\")\n",
        "print(f\"It will call the model at most {max_num_calls} times to the model\")\n",
        "print(f\"Previous methods will need up to {pervious_methods_max_num_calls} call to the model to genrate the same text\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 433
        },
        "id": "5X4jSTghn5fD",
        "outputId": "28e83a74-8846-4b3d-c548-0589417c073e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: the group size is bigger than the length of the model's output\n",
            "If the length of the model input (in tokens) is n, n will be length of the model's output\n",
            "the predicted text will be 1 tokens shorter\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-39-d15c40238ce9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcompletions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcomplete\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprompt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtop_p\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtop_k\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_groups\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroup_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcompletions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mgrouped_sampling_best_comp_tokenized\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcompletions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcompletions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-36-167b194b12da>\u001b[0m in \u001b[0;36mcomplete\u001b[0;34m(model_name, org_prompt, top_p, top_k, num_groups, group_size, org_prompt_prob)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m     \u001b[0mprob_mat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_prob_mat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr_prompt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroup_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m     \u001b[0mtokenized_ans_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrouped_sampling\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprob_mat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtop_p\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtop_k\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m     \u001b[0mprob_list\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mseq_prob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprob_mat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morg_prompt_prob\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mseq\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtokenized_ans_list\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m     \u001b[0mnew_prompts\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenized_prompt_list\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mans\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mans\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtokenized_ans_list\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-36-167b194b12da>\u001b[0m in \u001b[0;36mgrouped_sampling\u001b[0;34m(prob_mat, top_p, top_k)\u001b[0m\n\u001b[1;32m     59\u001b[0m                 \u001b[0mcurr_indices\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0mposible_tokens\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurr_indices\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# O(1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m     \u001b[0mnew_sequences\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcombinations\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mposible_tokens\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# theta(prod(len(indices[i]) for i in range(group_size)))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m     \u001b[0;31m# len(indices[i]) < min(top_k, vocab_size)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;31m# therefore the complexity is O(min(top_k, vocab_size) * group_size)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-36-167b194b12da>\u001b[0m in \u001b[0;36mcombinations\u001b[0;34m(mat)\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmat\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcombinations\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmat\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m             \u001b[0mres\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mfiltered_res\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoesnt_have_duplicates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-36-167b194b12da>\u001b[0m in \u001b[0;36mcombinations\u001b[0;34m(mat)\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmat\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcombinations\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmat\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m             \u001b[0mres\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mfiltered_res\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoesnt_have_duplicates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-36-167b194b12da>\u001b[0m in \u001b[0;36mcombinations\u001b[0;34m(mat)\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmat\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcombinations\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmat\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m             \u001b[0mres\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mfiltered_res\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoesnt_have_duplicates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "completions = complete(model_name, prompt, top_p, top_k, num_groups, group_size)\n",
        "\n",
        "\n",
        "if len(completions.keys()) > 0:\n",
        "    grouped_sampling_best_comp_tokenized = max(completions, key=completions.get)\n",
        "else:\n",
        "    raise ValueError(\"\"\"\n",
        "        The model could not generate a complition with the constrains of the sampling method, U should try:\n",
        "        1) Increase top p and/or top k\n",
        "        2) Use diffrent (larger) model\n",
        "        3) Decrease the number of groups\n",
        "        4) Decrease the group size\n",
        "        \"\"\")\n",
        "    \n",
        "grouped_sampling_best_comp = tokenizer.decode(grouped_sampling_best_comp_tokenized)\n",
        "print(\"The best text generated by grouped sampling is:\")\n",
        "print(grouped_sampling_best_comp)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5y1Tu90rpFu0"
      },
      "outputs": [],
      "source": [
        "sorted_complitions = sorted(completions, key = completions.get)\n",
        "print(\"Here are all the complitions created by predicted probability\")\n",
        "for tokens in sorted_complitions:\n",
        "    print(tokenizer.decode(tokens))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t8y5JMNk7wW9"
      },
      "source": [
        "# Change the hyper parameters to see what will happend!"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "\n",
        "# These are the usual ipython objects, including this one you are creating\n",
        "\n",
        "special_vars = {\"model\", \"tokenizer\", \"config\", 'In', 'Out', 'exit', 'quit', 'get_ipython', 'ipython_vars', \"g\"}\n",
        "\n",
        "def is_global_obj(my_obj_name):\n",
        "    if my_obj_name.startswith('_') or my_obj_name in sys.modules or my_obj_name in special_vars:\n",
        "        return False\n",
        "    return True\n",
        "\n",
        "\n",
        "# Get a sorted list of the objects and their sizes\n",
        "global_vars = [(x, sys.getsizeof(globals().get(x))) for x in dir() if is_global_obj(x)]\n",
        "sorted(global_vars, key=lambda x: x[1], reverse=True)"
      ],
      "metadata": {
        "id": "i2KFVYVKIJI3",
        "outputId": "effd0cf1-06bb-4735-8210-95607c69e842",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('special_vars', 744), ('is_global_obj', 144)]"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "final_project_demo.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3.9.12",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.12"
    },
    "vscode": {
      "interpreter": {
        "hash": "4c667eb41e29edd16195f2712b5916a29b36d4e6f17fe7c0a7edbb7a73b8ba8a"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}