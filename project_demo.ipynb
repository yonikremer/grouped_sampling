{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yonikremer/final_project/blob/master/project_demo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JHhVA_vxD6NV",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "# Demo - this is a demo for the project - \n",
        "\n",
        "Here yu can try out the sampling method I intreduced in my research.\n",
        "I highly reccomend to lookup [my research project](https://github.com/yonikremer/final_project) for more information."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yDbjm_yg5-5t",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "# Set Up"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "th_b4EHG4b4l",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "57ccb102-4d33-45d8-f775-93f0c6f4daf8",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K     |████████████████████████████████| 4.7 MB 22.8 MB/s \n",
            "\u001b[K     |████████████████████████████████| 6.6 MB 57.4 MB/s \n",
            "\u001b[K     |████████████████████████████████| 120 kB 72.9 MB/s \n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "%pip install -q transformers;\n",
        "%pip install -q torch;"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import List, Tuple, Dict, Optional, Union\n",
        "import random\n",
        "from copy import deepcopy\n",
        "import timeit\n",
        "from abc import ABC, abstractmethod\n",
        "from collections.abc import Callable\n",
        "from math import ceil"
      ],
      "metadata": {
        "id": "NTkbZjWS5pSc",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Rou3pmrewyXG",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM, AutoConfig, BatchEncoding\n",
        "\n",
        "import torch\n",
        "from torch import cuda, device, LongTensor, no_grad\n",
        "from torch.nn import Softmax"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "yNv-zShOcIPB",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "if not cuda.is_available():\n",
        "    print(\"Warning: CUDA not available\")\n",
        "    print(\"Running on CPU wil be very slow\")\n",
        "    print(\"it's highly recommended to use colab's GPU runtime\")\n",
        "    exit()\n",
        "else:\n",
        "    cuda = device('cuda')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dNzB89UE5anV",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "# Important Notes:\n",
        "\n",
        "\n",
        "*   Please make sure the model you choose is not too big for your hardware\n",
        "*   gpt2 is the smallest version of gpt-2, gpt2-xl is the largest\n",
        "*   bloom is the largest vesion of bloom\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7S5lrpOO0jS8",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "# The code I use to sample:"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Object Oriented way"
      ],
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "6v3Difu75fuQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "outputs": [],
      "source": [
        "class TextGenerator(Callable, ABC):\n",
        "    \"\"\"Generates text given a model, a prompt and some sampling parameters.\"\"\"\n",
        "    def __init__(self, model_name: str, group_size: int, temp: float = 1.0):\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "        self.model = AutoModelForCausalLM.from_pretrained(model_name).cuda()\n",
        "        self.vocab_size = AutoConfig.from_pretrained(model_name).vocab_size\n",
        "        self.temp = temp\n",
        "        self.group_size = group_size\n",
        "        pad_id = self.tokenizer.pad_token_id\n",
        "        self.padding_tokens = [pad_id for _ in range(self.group_size - 1)]\n",
        "\n",
        "\n",
        "    def get_prob_mat(self, prompt: Optional[str], token_list: Optional[List[int]] = None):\n",
        "        \"\"\"Returns the probability matrix as a list of lists of floats\"\"\"\n",
        "        attention_len = len(token_list) + self.group_size - 1\n",
        "        if token_list is None:\n",
        "            tokenenized_prompt = self.tokenizer(prompt, return_tensors=\"pt\")\n",
        "            if isinstance(tokenenized_prompt, list):\n",
        "                token_list = tokenenized_prompt\n",
        "            else:\n",
        "                token_list = tokenenized_prompt[\"input_ids\"]\n",
        "\n",
        "        longer_token_list = token_list + self.padding_tokens\n",
        "        longer_token_tensor = LongTensor([longer_token_list])\n",
        "        attention_mask = torch.ones([1, attention_len])\n",
        "        inputs = {\"input_ids\": longer_token_tensor, \"attention_mask\": attention_mask}\n",
        "        inputs = {name: tensor.cuda() for name, tensor in inputs.items()}\n",
        "\n",
        "        if not isinstance(inputs, dict):\n",
        "            with no_grad():\n",
        "                logits_pt_tensor = self.model(**inputs).logits.squeeze(0) / self.temp\n",
        "        elif not \"input_ids\" in inputs.keys():\n",
        "            with no_grad():\n",
        "                logits_pt_tensor = self.model(**inputs).logits.squeeze(0) / self.temp\n",
        "        else:\n",
        "            with no_grad():\n",
        "                logits_pt_tensor = self.model(**inputs, labels=inputs[\"input_ids\"]).logits.squeeze(0) / self.temp\n",
        "\n",
        "        prob_tensor = Softmax(dim=1)(logits_pt_tensor)\n",
        "        if self.group_size <= prob_tensor.shape[0]:\n",
        "            prob_tensor = prob_tensor[-self.group_size:, :]\n",
        "            prob_mat = [prob_tensor[i, :].tolist() for i in range(self.group_size)]\n",
        "        else:\n",
        "            print(\"Warning: the group size is bigger than the length of the model's output\")\n",
        "            print(\"If the length of the model input (in tokens) is n, n will be length of the model's output\")\n",
        "            print(f\"the predicted text will be {self.group_size - prob_tensor.shape[0]} tokens shorter\")\n",
        "            prob_mat = [prob_tensor[i, :].tolist() for i in range(prob_tensor.shape[0])]\n",
        "\n",
        "        return prob_mat\n",
        "\n",
        "\n",
        "    @abstractmethod\n",
        "    def __call__(self, prompt: str, num_new_tokens: int) -> str:\n",
        "        pass"
      ],
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "uLazxhj55fuR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "outputs": [],
      "source": [
        "class SampleGen(TextGenerator):\n",
        "    def __init__(self, model_name: str, group_size: int, top_k: int, top_p: float, temp: float = 1.0):\n",
        "        super().__init__(model_name, group_size, temp)\n",
        "        random.seed(0)\n",
        "        if top_p is None and top_k is not None:\n",
        "            self.top_k = top_k\n",
        "            self.sampling_method = \"top k\"\n",
        "        elif top_k is None and top_p is not None:\n",
        "            self.top_p = top_p\n",
        "            self.sampling_method = \"top p\"\n",
        "\n",
        "\n",
        "    def grouped_top_p_sampling(self, prob_mat: List[List[float]], org_used_tokens: List[int]):\n",
        "        used_tokens = deepcopy(org_used_tokens)\n",
        "        answer = []\n",
        "        for curr_token_prob_list in prob_mat:\n",
        "            for used_token in used_tokens:\n",
        "                curr_token_prob_list[used_token] = 0.0\n",
        "\n",
        "            indexed_prob: Dict[int, float] = {i: prob for i, prob in enumerate(curr_token_prob_list)}  # O(vocab_size)\n",
        "            sorted_items = sorted(indexed_prob.items(), key = lambda item: item[1], reverse=True)\n",
        "            sorted_indexed_prob = {key: value for key, value in sorted_items}\n",
        "            top_p_indexed_prob: Dict[int, float] = {}\n",
        "            prob_sum: float = 0.0\n",
        "            for i, (curr_token, curr_prob) in enumerate(sorted_indexed_prob.items()):\n",
        "                if i > 0 and prob_sum + curr_prob > self.top_p:\n",
        "                    break\n",
        "                prob_sum += curr_prob\n",
        "                top_p_indexed_prob[curr_token] = curr_prob\n",
        "\n",
        "            weighted_probs = {key: value / prob_sum for key, value in top_p_indexed_prob.items()}\n",
        "\n",
        "            sampled_token: int = random.choices(list(weighted_probs.keys()), weights = weighted_probs.values(), k=1)[0]\n",
        "            answer.append(sampled_token)\n",
        "            used_tokens.append(sampled_token)\n",
        "        return answer\n",
        "\n",
        "\n",
        "    def grouped_top_k_sampling(self, prob_mat: List[List[float]], org_used_tokens: List[int]):\n",
        "        used_tokens = deepcopy(org_used_tokens)\n",
        "        answer = []\n",
        "        for curr_token_prob_list in prob_mat:\n",
        "            for used_token in used_tokens:\n",
        "                curr_token_prob_list[used_token] = 0.0\n",
        "\n",
        "            indexed_prob: Dict[int, float] = {i: prob for i, prob in enumerate(curr_token_prob_list)}  # O(vocab_size)\n",
        "            sorted_items = sorted(indexed_prob.items(), key = lambda item: item[1], reverse=True)\n",
        "            sorted_indexed_prob = {key: value for key, value in sorted_items}\n",
        "            sorted_top_k_keys = list(sorted_indexed_prob.keys())[:self.top_k]\n",
        "            top_k_indexed_prob: Dict[int, float] = {key: sorted_indexed_prob[key] for key in sorted_top_k_keys}\n",
        "            prob_sum: float = sum(top_k_indexed_prob.values())\n",
        "            weighted_probs = {key: value / prob_sum for key, value in top_k_indexed_prob.items()}\n",
        "\n",
        "            sampled_token: int = random.choices(list(weighted_probs.keys()), weights = weighted_probs.values(), k=1)[0]\n",
        "            answer.append(sampled_token)\n",
        "            used_tokens.append(sampled_token)\n",
        "        return answer\n",
        "\n",
        "\n",
        "    def __call__(self, prompt: str, num_new_tokens: int) -> str:\n",
        "        num_groups = ceil(num_new_tokens / self.group_size)\n",
        "        tokenized_prompt_ten = self.tokenizer(prompt, return_tensors=\"pt\")\n",
        "        if isinstance(tokenized_prompt_ten, dict) or isinstance(tokenized_prompt_ten, BatchEncoding):\n",
        "            if \"input_ids\" in tokenized_prompt_ten.keys():\n",
        "                tokenized_prompt_ten = tokenized_prompt_ten[\"input_ids\"]\n",
        "\n",
        "        curr_token_list = tokenized_prompt_ten.squeeze().tolist()\n",
        "        for _ in range(num_groups):\n",
        "            prob_mat = self.get_prob_mat(None, curr_token_list)\n",
        "            if self.sampling_method == \"top k\":\n",
        "                new_tokens = self.grouped_top_k_sampling(prob_mat, curr_token_list)\n",
        "            elif self.sampling_method == \"top p\":\n",
        "                new_tokens = self.grouped_top_p_sampling(prob_mat, curr_token_list)\n",
        "            else:\n",
        "                raise ValueError(\"Either top_p ot top_k should be None, but not both\")\n",
        "            curr_token_list.extend(new_tokens)\n",
        "        final_num_tokens = tokenized_prompt_ten.shape[1] + num_new_tokens\n",
        "        shorten_token_list = curr_token_list[:final_num_tokens]\n",
        "        final_ans = self.tokenizer.decode(shorten_token_list)\n",
        "        return final_ans"
      ],
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "PWZZ3iFB5fua"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "outputs": [],
      "source": [
        "class TreeGen(SampleGen):\n",
        "    def __init__(self, model_name: str, group_size: int, top_k: int, top_p: float, temp: float = 1.0):\n",
        "        super().__init__(model_name, group_size, temp)\n",
        "        self.top_k = top_k\n",
        "        self.top_p = top_p\n",
        "\n",
        "\n",
        "    @staticmethod\n",
        "    def doesnt_have_duplicates(my_list):\n",
        "        \"\"\"Return if there isn't a repetition in the list\n",
        "        complexity: O(n) where n is the length of the list\"\"\"\n",
        "        return len(my_list) == len(set(my_list))\n",
        "\n",
        "\n",
        "    @staticmethod\n",
        "    def combinations(mat):\n",
        "        \"\"\"Returns all the lists such that list[j] is in mat[j]\n",
        "        complexity: prod([len(mat[i]) for i in range(len(mat))])\"\"\"\n",
        "        if len(mat) == 1:\n",
        "            return [[mat[0][i]] for i in range(len(mat[0]))]\n",
        "        res = []\n",
        "        for i in mat[0]:\n",
        "            for j in TreeGen.combinations(mat[1:]):\n",
        "                res.append([i] + j)\n",
        "        filtered_res = list(filter(TreeGen.doesnt_have_duplicates, res))\n",
        "        return filtered_res\n",
        "\n",
        "\n",
        "    @staticmethod\n",
        "    def seq_prob(tokens, prob_mat, org_prompt_prob) -> float:\n",
        "        \"\"\"Given the probability matrix and a list of tokens\n",
        "        returns the probability of the sequence\n",
        "        prob_mat[a][b] is the probability of the token with id b the a-th token in the sequence\"\"\"\n",
        "        probability = org_prompt_prob\n",
        "        sequence_length = len(tokens)\n",
        "        for i in range(sequence_length):\n",
        "            curr_token = tokens[i]\n",
        "            probability *= prob_mat[i][curr_token]\n",
        "        return probability\n",
        "\n",
        "\n",
        "    @staticmethod\n",
        "    def flatten(l: Union[list, tuple]) -> List:\n",
        "        \"\"\"Gets a list where some elements might be lists\n",
        "        and adds every item in the inner list to the outer list.\n",
        "        example: [1, [2, 3], 4, [[5]]] -> [1, 2, 3, 4, 5]\n",
        "        Complexity: O(len(the flatten list) + the number of diffrent lists))\"\"\"\n",
        "        new_list = []\n",
        "        for item in l:\n",
        "            if isinstance(item, list):\n",
        "                new_list.extend(TreeGen.flatten(item))\n",
        "            else:\n",
        "                new_list.append(item)\n",
        "        return new_list\n",
        "\n",
        "\n",
        "    @staticmethod\n",
        "    def remove_duplicates(completions: List[List[int]], probs: List[float]) -> Dict[Tuple[int], float]:\n",
        "        \"\"\"Given a list of tokenized answers and the probability of each completion,\n",
        "        removes every repeated completion and every completion that have repeated tokens\"\"\"\n",
        "        filtered_completions: Dict[Tuple[int], float] = dict()\n",
        "        for curr_comp, curr_prob in zip(completions, probs):\n",
        "            try:\n",
        "                cond = len(curr_comp) == len(set(curr_comp))\n",
        "            except TypeError as e:\n",
        "                print(curr_comp)\n",
        "                raise e\n",
        "            if cond:\n",
        "                curr_comp_tuple = tuple(curr_comp)\n",
        "                filtered_completions[curr_comp_tuple] = curr_prob\n",
        "        return filtered_completions\n",
        "\n",
        "\n",
        "    def tree_grouped_sampling(self, prob_mat: List[List[float]]) -> List[List[int]]:\n",
        "        \"\"\"given a matrix of probabilities, returns a list of lists of tokens\n",
        "        the matrix's is of size group_size x vocab_size\n",
        "        where matrix[i, j] is the probability of token j the i-th token in the group\n",
        "        samples the tokens such that for each place in the group,\n",
        "        at most top_k tokens are sampled and at least one token is sampled\n",
        "        and the added probability of all the tokens is less than or equal top_p\n",
        "        returns a list of where every item is a tuple of a sequense and probability\n",
        "        over all complexity of the function in O(group_size * vocab_size * log(vocab_size))\"\"\"\n",
        "\n",
        "        # prob_tensor.shape is now (group_size, vocab_size)\n",
        "        possible_tokens = []\n",
        "        already_predicted = set()\n",
        "        for token_prob in prob_mat: # group_size times\n",
        "            vocab_size = len(token_prob)  # O(1)\n",
        "            indexed_prob = list(zip(token_prob, range(vocab_size)))  # O(vocab_size)\n",
        "            sorted_indexed_prob = sorted(indexed_prob, key=lambda x: x[0], reverse=True) # O(vocab_size*log(vocab_size))\n",
        "            curr_k = 0\n",
        "            total_prob = 0\n",
        "            curr_indices = []\n",
        "            for prob, token in sorted_indexed_prob:  # O(top_k)\n",
        "                if total_prob + prob > self.top_p or curr_k == self.top_k:\n",
        "                    break\n",
        "                if not token in already_predicted:\n",
        "                    already_predicted.add(token)\n",
        "                    curr_k += 1\n",
        "                    total_prob += prob\n",
        "                    curr_indices.append(token)\n",
        "            possible_tokens.append(curr_indices)  # O(1)\n",
        "        new_sequences: List[List[int]] = TreeGen.combinations(possible_tokens)\n",
        "        # theta(prod(len(indices[i]) for i in range(group_size)))\n",
        "        # len(indices[i]) < min(top_k, vocab_size)\n",
        "        # therefore the complexity is O(min(top_k, vocab_size) * group_size)\n",
        "        return new_sequences\n",
        "\n",
        "\n",
        "    def rec_gen(self, org_prompt, num_tokens: int, org_prompt_prob: float = 1.0) -> Dict[Tuple[int], float]:\n",
        "        \"\"\"Recursively generates the next group of tokens in a tree like behavior\"\"\"\n",
        "        num_groups = ceil(num_tokens / self.group_size)\n",
        "        if isinstance(org_prompt, list) or isinstance(org_prompt, tuple):\n",
        "            tokenized_prompt_list = TreeGen.flatten(org_prompt)\n",
        "            str_prompt = self.tokenizer.decode(tokenized_prompt_list)\n",
        "        else:\n",
        "            print(org_prompt)\n",
        "            raise ValueError(\"org_prompt must be a string or list of integers\")\n",
        "\n",
        "        prob_mat = TreeGen.get_prob_mat(str_prompt, group_size)\n",
        "        tokenized_ans_list = TreeGen.tree_grouped_sampling(prob_mat)\n",
        "        prob_list: List[float] = [TreeGen.seq_prob(seq, prob_mat, org_prompt_prob) for seq in tokenized_ans_list]\n",
        "        new_prompts: List[List[int]] = [TreeGen.flatten(tokenized_prompt_list + ans) for ans in tokenized_ans_list]\n",
        "        completion_prob_dict: Dict[Tuple[int], float] = TreeGen.remove_duplicates(new_prompts, prob_list)\n",
        "        if num_groups == 1:\n",
        "            shorten_completions = {k[:num_tokens]: v for k, v in completion_prob_dict.items()}\n",
        "            return shorten_completions\n",
        "        new_completions: Dict[Tuple[int], float] = dict()\n",
        "        for curr_new_prompt, curr_new_prompt_prob in completion_prob_dict.items():\n",
        "            curr_completions: Dict[Tuple[int], float] = self.rec_gen(curr_new_prompt, num_tokens - self.group_size, curr_new_prompt_prob)\n",
        "            tokens: Tuple[int]\n",
        "            prob: float\n",
        "            for tokens, prob in curr_completions.items():\n",
        "                new_completions[tokens] = prob\n",
        "        return new_completions\n",
        "\n",
        "\n",
        "    def __call__(self, prompt: str, num_new_tokens: int) -> str:\n",
        "        tokenized_prompt_ten = self.tokenizer(prompt, return_tensors=\"pt\")\n",
        "        if isinstance(tokenized_prompt_ten, dict) or isinstance(tokenized_prompt_ten, BatchEncoding):\n",
        "            if \"input_ids\" in tokenized_prompt_ten.keys():\n",
        "                tokenized_prompt_ten = tokenized_prompt_ten[\"input_ids\"]\n",
        "        \n",
        "        final_num_tokens = tokenized_prompt_ten.shape[1] + num_new_tokens\n",
        "\n",
        "        tokenized_prompt: List[int] = tokenized_prompt_ten.tolist()\n",
        "        seq_prob_dict: Dict[Tuple[int], float] = self.rec_gen(tokenized_prompt, num_new_tokens)\n",
        "        highest_prob_seq: Tuple[int] = max(seq_prob_dict, key=seq_prob_dict.get)\n",
        "        decoded_prompt = self.tokenizer.decode(highest_prob_seq[:final_num_tokens])\n",
        "        return decoded_prompt"
      ],
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "V3eOWH7j5fug"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "outputs": [],
      "source": [
        "def compare_generators(grouped_generator: TextGenerator, non_grouped_generator: TextGenerator, prompt: str, num_tokens: int):\n",
        "    \"\"\"Compares grouped and non-grouped text generators\"\"\"\n",
        "    print(f\"Your prompt:\")\n",
        "    print(prompt)\n",
        "\n",
        "    start_non_grouped = timeit.default_timer()\n",
        "    non_grouped_ans: str = non_grouped_generator(prompt, num_tokens)\n",
        "    stop_non_grouped = timeit.default_timer()\n",
        "    non_grouped_time = stop_non_grouped - start_non_grouped\n",
        "    print(f\"Text generated by Non grouped sampling in {non_grouped_time} seconds:\")\n",
        "    print(non_grouped_ans)\n",
        "\n",
        "    start_grouped_generation = timeit.default_timer()\n",
        "    grouped_ans: str = grouped_generator(prompt, num_tokens)\n",
        "    stop_grouped_generation = timeit.default_timer()\n",
        "    grouped_time = stop_grouped_generation - start_grouped_generation\n",
        "    print(f\"Text generated by grouped sampling in {grouped_time} seconds:\")\n",
        "    print(grouped_ans)"
      ],
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "lmHn6h-45fuj"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pDOGorUX0pFg",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "# Use grouped sampling:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "9BUwTv4reNg_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f9937e8f-5565-429d-84fa-411ef9d3054f",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The model will generate 4 tokens (words or parts of words)\n",
            "It will call the model at most 2 times to the model\n",
            "Previous methods will need up to 4 call to the model to generate the same text\n",
            "Your prompt:\n",
            "I was so sad to graduate high school that I almost \n",
            "Text generated by Non grouped sampling in 2.255551921999995 seconds:\n",
            "I was so sad to graduate high school that I almost  couldn’t stand it\n",
            "Text generated by grouped sampling in 1.18751023599998 seconds:\n",
            "I was so sad to graduate high school that I almost  missed did itt\n"
          ]
        }
      ],
      "source": [
        "model_name = \"bigscience/bloom-1b1\" #@param [\"facebook/opt-125m\", \"facebook/opt-1.3b\", \"facebook/opt-350m\", \"gpt2\", \"gpt2-medium\", \"gpt2-large\", \"bigscience/bloom-1b1\", \"bigscience/bloom-560m\"]\n",
        "prompt = \"I was so sad to graduate high school that I almost \" #@param {type:\"string\"}\n",
        "num_tokens = 4 #@param {type:\"integer\", min:1}\n",
        "top_p = 0.4 #@param {type:\"slider\", min:0.0, max:1.0, step:0.05}\n",
        "top_k = 10000000000000000000000 #@param {type:\"integer\"}\n",
        "group_size = 2 #@param {type:\"integer\"}\n",
        "temperature = 1 #@param {type:\"number\", min:0.000000001}\n",
        "sampling_type = 'from distribution' #@param [\"from distribution\", \"as tree\"]\n",
        "\n",
        "\n",
        "vocab_size = AutoConfig.from_pretrained(model_name).vocab_size\n",
        "\n",
        "num_groups = ceil(num_tokens / group_size)\n",
        "actual_top_k = min(top_k, vocab_size * top_p)\n",
        "\n",
        "if sampling_type == \"as tree\":\n",
        "    max_num_calls = sum((actual_top_k ** i) for i in range(num_groups))    \n",
        "    previous_methods_max_num_calls = sum((actual_top_k ** i) for i in range(num_tokens))\n",
        "    grouped_generator = TreeGen(model_name, group_size, top_k, top_p, temperature)\n",
        "    non_grouped_generator = TreeGen(model_name, 1, top_k, top_p, temperature)\n",
        "else:\n",
        "    max_num_calls = num_groups\n",
        "    previous_methods_max_num_calls = num_tokens\n",
        "    if top_p == 1.0:\n",
        "        top_p = None\n",
        "    elif top_p == 0.0:\n",
        "        top_k = 1\n",
        "        top_p = None\n",
        "    elif top_k >= vocab_size:\n",
        "        top_k = None\n",
        "    else:\n",
        "        raise ValueError(\"When using sampling from distribution, You must use either top k or top k and no both\")\n",
        "    grouped_generator = SampleGen(model_name, group_size, top_k, top_p, temperature)\n",
        "    non_grouped_generator = SampleGen(model_name, 1, top_k, top_p, temperature)\n",
        "\n",
        "print(f\"The model will generate {num_tokens} tokens (words or parts of words)\")\n",
        "print(f\"It will call the model at most {max_num_calls} times to the model\")\n",
        "print(f\"Previous methods will need up to {previous_methods_max_num_calls} call to the model to generate the same text\")\n",
        "\n",
        "compare_generators(grouped_generator, non_grouped_generator, prompt, num_tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t8y5JMNk7wW9",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "# Change the hyper-parameters to see what will happen!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "([170675], 'PAD')"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ],
      "source": [
        "global_tokenizer = non_grouped_generator.tokenizer\n"
      ],
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "cHNl59Iq5fup",
        "outputId": "8b07fc0a-01fa-44d0-811b-0e7db73baca6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_prompt_ten = global_tokenizer(prompt, return_tensors=\"pt\")\n",
        "if isinstance(tokenized_prompt_ten, dict) or isinstance(tokenized_prompt_ten, BatchEncoding):\n",
        "    if \"input_ids\" in tokenized_prompt_ten.keys():\n",
        "        tokenized_prompt_ten = tokenized_prompt_ten[\"input_ids\"]\n",
        "tokenized_prompt_ten.shape[1]"
      ],
      "metadata": {
        "id": "nVpEW34JebFA",
        "outputId": "19fc3857-f735-4989-95c6-74798baa91bc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "12"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "final_project_demo.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3.9.12",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.12"
    },
    "vscode": {
      "interpreter": {
        "hash": "4c667eb41e29edd16195f2712b5916a29b36d4e6f17fe7c0a7edbb7a73b8ba8a"
      }
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}