{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "zfYZCJk8vMFF"
      },
      "source": [
        "# Demo - this is a demo for the project - \n",
        "\n",
        "Here yu can try out the sampling method I intreduced in my research.\n",
        "I highly reccomend to lookup [my research project](https://github.com/yonikremer/final_project) for more information."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "CvGtHXSAvMFV"
      },
      "source": [
        "# Set Up"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "kt0oGHBRvMFY"
      },
      "outputs": [],
      "source": [
        "import timeit, sys\n",
        "from math import ceil"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "eARBw0ycvMFm",
        "outputId": "e3272a36-a407-4c15-eaf1-50967350a6a2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l\r\u001b[K     |█                               | 10 kB 21.7 MB/s eta 0:00:01\r\u001b[K     |██▏                             | 20 kB 8.7 MB/s eta 0:00:01\r\u001b[K     |███▎                            | 30 kB 12.2 MB/s eta 0:00:01\r\u001b[K     |████▍                           | 40 kB 7.3 MB/s eta 0:00:01\r\u001b[K     |█████▌                          | 51 kB 6.2 MB/s eta 0:00:01\r\u001b[K     |██████▋                         | 61 kB 7.2 MB/s eta 0:00:01\r\u001b[K     |███████▊                        | 71 kB 8.2 MB/s eta 0:00:01\r\u001b[K     |████████▉                       | 81 kB 9.2 MB/s eta 0:00:01\r\u001b[K     |██████████                      | 92 kB 9.1 MB/s eta 0:00:01\r\u001b[K     |███████████                     | 102 kB 8.4 MB/s eta 0:00:01\r\u001b[K     |████████████                    | 112 kB 8.4 MB/s eta 0:00:01\r\u001b[K     |█████████████▏                  | 122 kB 8.4 MB/s eta 0:00:01\r\u001b[K     |██████████████▎                 | 133 kB 8.4 MB/s eta 0:00:01\r\u001b[K     |███████████████▍                | 143 kB 8.4 MB/s eta 0:00:01\r\u001b[K     |████████████████▌               | 153 kB 8.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████▋              | 163 kB 8.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████▊             | 174 kB 8.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████▉            | 184 kB 8.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████▉           | 194 kB 8.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 204 kB 8.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 215 kB 8.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▏       | 225 kB 8.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▎      | 235 kB 8.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▍     | 245 kB 8.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▌    | 256 kB 8.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▋   | 266 kB 8.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▊  | 276 kB 8.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▊ | 286 kB 8.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▉| 296 kB 8.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 298 kB 8.4 MB/s \n",
            "\u001b[K     |████████████████████████████████| 101 kB 9.7 MB/s \n",
            "\u001b[K     |████████████████████████████████| 96 kB 5.6 MB/s \n",
            "\u001b[K     |████████████████████████████████| 232 kB 63.1 MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.2 MB 46.8 MB/s \n",
            "\u001b[K     |████████████████████████████████| 578.0 MB 15 kB/s \n",
            "\u001b[K     |████████████████████████████████| 5.9 MB 33.0 MB/s \n",
            "\u001b[31mERROR: Could not find a version that satisfies the requirement pandas==1.4.4 (from versions: 0.1, 0.2, 0.3.0, 0.4.0, 0.4.1, 0.4.2, 0.4.3, 0.5.0, 0.6.0, 0.6.1, 0.7.0, 0.7.1, 0.7.2, 0.7.3, 0.8.0, 0.8.1, 0.9.0, 0.9.1, 0.10.0, 0.10.1, 0.11.0, 0.12.0, 0.13.0, 0.13.1, 0.14.0, 0.14.1, 0.15.0, 0.15.1, 0.15.2, 0.16.0, 0.16.1, 0.16.2, 0.17.0, 0.17.1, 0.18.0, 0.18.1, 0.19.0, 0.19.1, 0.19.2, 0.20.0, 0.20.1, 0.20.2, 0.20.3, 0.21.0, 0.21.1, 0.22.0, 0.23.0, 0.23.1, 0.23.2, 0.23.3, 0.23.4, 0.24.0, 0.24.1, 0.24.2, 0.25.0, 0.25.1, 0.25.2, 0.25.3, 1.0.0, 1.0.1, 1.0.2, 1.0.3, 1.0.4, 1.0.5, 1.1.0, 1.1.1, 1.1.2, 1.1.3, 1.1.4, 1.1.5, 1.2.0, 1.2.1, 1.2.2, 1.2.3, 1.2.4, 1.2.5, 1.3.0, 1.3.1, 1.3.2, 1.3.3, 1.3.4, 1.3.5)\u001b[0m\n",
            "\u001b[31mERROR: No matching distribution found for pandas==1.4.4\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "%pip install -q -r /content/final_project/requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "XAu7MQGKvMFj"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/yonikremer/final_project.git\n",
        "sys.path.append('/content/final_project')\n",
        "from final_project.text_generator import TextGenerator\n",
        "from final_project.tree_generator import TreeGen\n",
        "from final_project.sample_generator import SampleGen"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "UTZECwuDvMFo"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoConfig, BatchEncoding\n",
        "from torch import cuda, device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "VlYhu98rvMFq"
      },
      "outputs": [],
      "source": [
        "if not cuda.is_available():\n",
        "    print(\"Warning: CUDA not available\")\n",
        "    print(\"Running on CPU wil be very slow\")\n",
        "    print(\"it's highly recommended to use colab's GPU runtime\")\n",
        "    exit()\n",
        "else:\n",
        "    cuda = device('cuda')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "Xm8N9zCSvMFu"
      },
      "source": [
        "# Important Notes:\n",
        "\n",
        "\n",
        "*   Please make sure the model you choose is not too big for your hardware\n",
        "*   gpt2 is the smallest version of gpt-2, gpt2-xl is the largest\n",
        "*   bloom is the largest vesion of bloom\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "bM_vYqatvMFz"
      },
      "outputs": [],
      "source": [
        "def compare_generators(grouped_generator: TextGenerator, non_grouped_generator: TextGenerator, prompt: str, num_tokens: int):\n",
        "    \"\"\"Compares grouped and non-grouped text generators\"\"\"\n",
        "    print(f\"Your prompt:\")\n",
        "    print(prompt)\n",
        "\n",
        "    start_non_grouped = timeit.default_timer()\n",
        "    non_grouped_ans: str = non_grouped_generator(prompt, num_tokens)\n",
        "    stop_non_grouped = timeit.default_timer()\n",
        "    non_grouped_time = stop_non_grouped - start_non_grouped\n",
        "    print(f\"Text generated by Non grouped sampling in {non_grouped_time} seconds:\")\n",
        "    print(non_grouped_ans)\n",
        "\n",
        "    start_grouped_generation = timeit.default_timer()\n",
        "    grouped_ans: str = grouped_generator(prompt, num_tokens)\n",
        "    stop_grouped_generation = timeit.default_timer()\n",
        "    grouped_time = stop_grouped_generation - start_grouped_generation\n",
        "    print(f\"Text generated by grouped sampling in {grouped_time} seconds:\")\n",
        "    print(grouped_ans)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "u3Z6Y8zjvMF3"
      },
      "source": [
        "# Use grouped sampling:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "NQtDCVYAvMF5"
      },
      "outputs": [],
      "source": [
        "model_name = \"bigscience/bloom-1b1\" #@param [\"facebook/opt-125m\", \"facebook/opt-1.3b\", \"facebook/opt-350m\", \"gpt2\", \"gpt2-medium\", \"gpt2-large\", \"bigscience/bloom-1b1\", \"bigscience/bloom-560m\"]\n",
        "prompt = \"I was so sad to graduate high school that I almost \" #@param {type:\"string\"}\n",
        "num_tokens = 4 #@param {type:\"integer\", min:1}\n",
        "top_p = 0.4 #@param {type:\"slider\", min:0.0, max:1.0, step:0.05}\n",
        "top_k = 10000000000000000000000 #@param {type:\"integer\"}\n",
        "group_size = 2 #@param {type:\"integer\"}\n",
        "temperature = 1 #@param {type:\"number\", min:0.000000001}\n",
        "sampling_type = 'from distribution' #@param [\"from distribution\", \"as tree\"]\n",
        "\n",
        "\n",
        "vocab_size = AutoConfig.from_pretrained(model_name).vocab_size\n",
        "\n",
        "num_groups = ceil(num_tokens / group_size)\n",
        "actual_top_k = min(top_k, vocab_size * top_p)\n",
        "\n",
        "if sampling_type == \"as tree\":\n",
        "    max_num_calls = sum((actual_top_k ** i) for i in range(num_groups))    \n",
        "    previous_methods_max_num_calls = sum((actual_top_k ** i) for i in range(num_tokens))\n",
        "    grouped_generator = TreeGen(model_name, group_size, top_k, top_p, temperature)\n",
        "    non_grouped_generator = TreeGen(model_name, 1, top_k, top_p, temperature)\n",
        "else:\n",
        "    max_num_calls = num_groups\n",
        "    previous_methods_max_num_calls = num_tokens\n",
        "    if top_p == 1.0:\n",
        "        top_p = None\n",
        "    elif top_p == 0.0:\n",
        "        top_k = 1\n",
        "        top_p = None\n",
        "    elif top_k >= vocab_size:\n",
        "        top_k = None\n",
        "    else:\n",
        "        raise ValueError(\"When using sampling from distribution, You must use either top k or top k and no both\")\n",
        "    grouped_generator = SampleGen(model_name, group_size, top_k, top_p, temperature)\n",
        "    non_grouped_generator = SampleGen(model_name, 1, top_k, top_p, temperature)\n",
        "\n",
        "print(f\"The model will generate {num_tokens} tokens (words or parts of words)\")\n",
        "print(f\"It will call the model at most {max_num_calls} times to the model\")\n",
        "print(f\"Previous methods will need up to {previous_methods_max_num_calls} call to the model to generate the same text\")\n",
        "\n",
        "compare_generators(grouped_generator, non_grouped_generator, prompt, num_tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "aPUXtswQvMF9"
      },
      "source": [
        "# Change the hyper-parameters to see what will happen!"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "final_project_demo.ipynb",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3.9.12",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.12"
    },
    "vscode": {
      "interpreter": {
        "hash": "4c667eb41e29edd16195f2712b5916a29b36d4e6f17fe7c0a7edbb7a73b8ba8a"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}