{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Imports","metadata":{"id":"ZZf-fbj50Kqc"}},{"cell_type":"code","source":"def install_libaries():\n    %pip install -q gensim==3.6.0\n    %pip install -q nlppreprocess\n    %pip install -q keras\n    %pip install -q keras_preprocessing","metadata":{"execution":{"iopub.status.busy":"2022-07-13T15:41:01.005565Z","iopub.execute_input":"2022-07-13T15:41:01.006126Z","iopub.status.idle":"2022-07-13T15:41:01.015327Z","shell.execute_reply.started":"2022-07-13T15:41:01.006087Z","shell.execute_reply":"2022-07-13T15:41:01.013664Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import sys\ndevice: str\nif \"google.colab\" in sys.modules.keys():\n    device = \"colab\"\nif \"kaggle_web_client\" in sys.modules.keys():\n    device = \"kaggle\"\nelse:\n    device = \"locally\"","metadata":{"execution":{"iopub.status.busy":"2022-07-13T15:41:03.021477Z","iopub.execute_input":"2022-07-13T15:41:03.021908Z","iopub.status.idle":"2022-07-13T15:41:03.028135Z","shell.execute_reply.started":"2022-07-13T15:41:03.021872Z","shell.execute_reply":"2022-07-13T15:41:03.027141Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# standart liberies:\nfrom typing import Optional, List, Set, Dict, Tuple\nimport datetime, os, random, statistics, math, time\n# NON-standart liberies:\nimport wandb\nimport tensorflow as tf\nimport pandas as pd\nimport seaborn as sns\nimport numpy as np\nimport tqdm\nimport pandas as pd\ntry:\n    import keras\n    from keras import layers, Model\n    from keras_preprocessing.text import Tokenizer\n    from keras_preprocessing.sequence import pad_sequences\n    from nlppreprocess import NLP as nlp\n    from gensim.scripts.glove2word2vec import glove2word2vec\n    from gensim.models.keyedvectors import KeyedVectors\nexcept ModuleNotFoundError as e:\n    print(e)\n    install_libaries()\nexcept ImportError as e:\n    print(e)\n    install_libaries()","metadata":{"gather":{"logged":1644854036589},"id":"MsfMr-Qod_nl","outputId":"e09897f8-e611-4818-b6b6-adf1e370cccf","execution":{"iopub.status.busy":"2022-07-13T15:41:07.605026Z","iopub.execute_input":"2022-07-13T15:41:07.605981Z","iopub.status.idle":"2022-07-13T15:41:15.278393Z","shell.execute_reply.started":"2022-07-13T15:41:07.605942Z","shell.execute_reply":"2022-07-13T15:41:15.276793Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f\"Python version: {sys.version}\")\nprint(f\"Tensorflow version: {tf.__version__}\")","metadata":{"gather":{"logged":1644854037100},"id":"SzaglEQ-kp2n","outputId":"5b7fd3b9-3369-431c-b516-bde95519988f","pycharm":{"name":"#%%\n"},"execution":{"iopub.status.busy":"2022-07-13T15:41:15.280887Z","iopub.execute_input":"2022-07-13T15:41:15.281569Z","iopub.status.idle":"2022-07-13T15:41:15.288533Z","shell.execute_reply.started":"2022-07-13T15:41:15.281527Z","shell.execute_reply":"2022-07-13T15:41:15.287142Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Settings","metadata":{"id":"91ZJ0-RZIx6j"}},{"cell_type":"code","source":"tf.random.set_seed(0)\nrandom.seed(0)\nnp.random.seed(0)\n# keras.utils.set_random_seed(0)\n# tf.config.experimental.enable_op_determinism()\n# keras.backend.set_floatx(\"float16\")\nf_type = keras.backend.floatx()  # either tf.float16 or tf.float32","metadata":{"gather":{"logged":1644854038983},"id":"2mp2yFTEkp2u","pycharm":{"name":"#%%\n"},"execution":{"iopub.status.busy":"2022-07-13T15:41:15.861154Z","iopub.execute_input":"2022-07-13T15:41:15.861574Z","iopub.status.idle":"2022-07-13T15:41:15.868020Z","shell.execute_reply.started":"2022-07-13T15:41:15.861540Z","shell.execute_reply":"2022-07-13T15:41:15.866925Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Define a strategy - Accelerator optimization ","metadata":{"id":"QlubPo7LRyNm"}},{"cell_type":"code","source":"try:\n    resolver = tf.distribute.cluster_resolver.TPUClusterResolver();\n    tf.config.experimental_connect_to_cluster(resolver);\n    tf.tpu.experimental.initialize_tpu_system(resolver);\n    strategy = tf.distribute.TPUStrategy(resolver);\n    using_tpu = True\nexcept ValueError:\n    using_tpu = False","metadata":{"id":"s0yqg12zqSt9","execution":{"iopub.status.busy":"2022-07-13T15:41:18.742547Z","iopub.execute_input":"2022-07-13T15:41:18.745558Z","iopub.status.idle":"2022-07-13T15:41:18.765173Z","shell.execute_reply.started":"2022-07-13T15:41:18.745301Z","shell.execute_reply":"2022-07-13T15:41:18.761782Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data loading","metadata":{"id":"yVR1oJRD0NtO","_kg_hide-input":true}},{"cell_type":"code","source":"if device == \"colab\":  # If notebook is ran on colab\n    from google.colab import drive\n    drive.mount(\"/drive\")\n    df: pd.DataFrame = pd.read_csv(\"/drive/MyDrive/final_project/wikipedia_articles.csv\")\nelif device == \"kaggle\":\n    df: pd.DataFrame = pd.read_csv(\"../input/wikipedia-promotional-articles/promotional.csv\")\nelse:  # If notebook is ran on my laptop\n    df: pd.DataFrame = pd.read_csv(\"wiki_data/articles.csv\")\nprint(f\"the shape of the dataframe: {df.shape}\")\n\ntrain_df = df.sample(frac=0.8, random_state=0) #random state is a seed value\ntemp_val_df = df.drop(train_df.index)\ntest_df = temp_val_df.sample(frac=0.5, random_state=0)\nval_df = temp_val_df.drop(test_df.index)\n\ntrain_ser = train_df[\"text\"]\nval_ser = val_df[\"text\"]\ntest_ser = test_df[\"text\"]\n\ntrain_ser.shape, val_ser.shape, test_ser.shape","metadata":{"gather":{"logged":1644857858611},"id":"pn1UfAXQwlQX","outputId":"af20cbcb-3d71-4d4b-be9a-5850b9f9741c","execution":{"iopub.status.busy":"2022-07-13T15:41:21.779653Z","iopub.execute_input":"2022-07-13T15:41:21.782696Z","iopub.status.idle":"2022-07-13T15:41:25.391805Z","shell.execute_reply.started":"2022-07-13T15:41:21.782418Z","shell.execute_reply":"2022-07-13T15:41:25.384059Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Creating a tokenizer","metadata":{}},{"cell_type":"code","source":"oov_token = \"[OOV]\"\ntokenizer = Tokenizer(num_words=8192, oov_token=oov_token)\ntokenizer.fit_on_texts(list((train_ser.apply(nlp().process).values)))\nlist_tokenized_train: List[List[int]] = tokenizer.texts_to_sequences(train_ser.values)\nlist_tokenized_val: List[List[int]] = tokenizer.texts_to_sequences(val_ser.values)\nlist_tokenized_test: List[List[int]] = tokenizer.texts_to_sequences(test_ser.values)\nvocab_size = tokenizer.get_config()[\"num_words\"]","metadata":{"execution":{"iopub.status.busy":"2022-07-13T15:41:25.405341Z","iopub.execute_input":"2022-07-13T15:41:25.411462Z","iopub.status.idle":"2022-07-13T15:42:02.357694Z","shell.execute_reply.started":"2022-07-13T15:41:25.411130Z","shell.execute_reply":"2022-07-13T15:42:02.356360Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenizer.texts_to_sequences([\"wqe\"])","metadata":{"execution":{"iopub.status.busy":"2022-07-13T15:42:02.359459Z","iopub.execute_input":"2022-07-13T15:42:02.359803Z","iopub.status.idle":"2022-07-13T15:42:02.367380Z","shell.execute_reply.started":"2022-07-13T15:42:02.359773Z","shell.execute_reply":"2022-07-13T15:42:02.366149Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Tokenizing the data","metadata":{"id":"A73Lbgg6kp3R"}},{"cell_type":"markdown","source":"### chunk too long texts","metadata":{"id":"EtjxgLWMkp3l"}},{"cell_type":"code","source":"max_seq_len: int = 256 if using_tpu else 64\n\ndef chunk_double_list(mat: List[List[int]], max_len: int = max_seq_len) -> List[List[int]]:\n    \"\"\"Splits token list to chunks (lists) of maximum size: max_len\"\"\"\n    chunked_mat = []\n    for l in mat:\n        chunked_mat += [l[i*max_len:(i+1)*max_len] for i in range(len(l) // max_len)]\n    return list(filter(lambda x: len(x) > 0, chunked_mat))","metadata":{"id":"MubdHBEMkJNC","execution":{"iopub.status.busy":"2022-07-13T15:42:02.369973Z","iopub.execute_input":"2022-07-13T15:42:02.370353Z","iopub.status.idle":"2022-07-13T15:42:02.378504Z","shell.execute_reply.started":"2022-07-13T15:42:02.370309Z","shell.execute_reply":"2022-07-13T15:42:02.377337Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"chunked_train: List[List[int]] = chunk_double_list(list_tokenized_train)\nchunked_val: List[List[int]] = chunk_double_list(list_tokenized_val)\nchunked_test: List[List[int]] = chunk_double_list(list_tokenized_test)","metadata":{"id":"9DgU2Or2kp3m","outputId":"596803b2-a306-4ef7-92c0-a497ecc507e3","pycharm":{"name":"#%%\n"},"execution":{"iopub.status.busy":"2022-07-13T15:42:02.380276Z","iopub.execute_input":"2022-07-13T15:42:02.380754Z","iopub.status.idle":"2022-07-13T15:42:03.560630Z","shell.execute_reply.started":"2022-07-13T15:42:02.380717Z","shell.execute_reply":"2022-07-13T15:42:03.559356Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Padding","metadata":{"id":"R8Z2_P2Lnke8"}},{"cell_type":"code","source":"chunked_train.sort(key = lambda l: len(l))  # sorting so that every batch will have similar sized texts, used when training\nchunked_val.sort(key = lambda l: len(l))\nchunked_test.sort(key = lambda l: len(l))\n\npad_int: int = 0\npad_ten = tf.constant(pad_int, dtype=tf.int32)\n    \npadded_train: tf.Tensor = pad_sequences(chunked_train, padding=\"pre\", value=pad_int)\npadded_val: tf.Tensor = pad_sequences(chunked_val, padding=\"pre\", value=pad_int)\npadded_test: tf.Tensor = pad_sequences(chunked_test, padding=\"pre\", value=pad_int)","metadata":{"id":"nRa9_ryHkp3p","pycharm":{"name":"#%%\n"},"execution":{"iopub.status.busy":"2022-07-13T15:42:03.562383Z","iopub.execute_input":"2022-07-13T15:42:03.562770Z","iopub.status.idle":"2022-07-13T15:42:05.201530Z","shell.execute_reply.started":"2022-07-13T15:42:03.562726Z","shell.execute_reply":"2022-07-13T15:42:05.200383Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Train test val split","metadata":{"id":"gbnub0UZkp3s"}},{"cell_type":"code","source":"batch_size: int = 128 if using_tpu else 4\n    \ndef ten_to_dataset(tokenized_ten: tf.Tensor) -> tf.data.Dataset:\n    \"\"\"Converts a list of tokenized texts after all preprocessing to a tf.data.Dataset\"\"\"\n    dataset: tf.data.Dataset = tf.data.Dataset.from_tensor_slices(tokenized_ten)\n    dataset = dataset.batch(batch_size)\n    return dataset\n\ntrain_dataset = ten_to_dataset(padded_train)\nval_dataset = ten_to_dataset(padded_val)\ntest_dataset = ten_to_dataset(padded_test)\n\nlist_train_set = list(train_dataset)\nlist_val_set = list(val_dataset)\nlist_test_set = list(test_dataset)","metadata":{"id":"pTW48b8Qr6sv","execution":{"iopub.status.busy":"2022-07-13T15:42:05.203266Z","iopub.execute_input":"2022-07-13T15:42:05.203646Z","iopub.status.idle":"2022-07-13T15:42:11.763595Z","shell.execute_reply.started":"2022-07-13T15:42:05.203612Z","shell.execute_reply":"2022-07-13T15:42:11.762336Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Clear memory","metadata":{"id":"HzS72rcIvL0N"}},{"cell_type":"code","source":"del train_dataset, val_dataset, test_dataset\ndel chunked_train, chunked_val, chunked_test\ndel padded_train, padded_val, padded_test\ndel list_tokenized_train, list_tokenized_val, list_tokenized_test\ndel train_df, temp_val_df, val_df, test_df\ndel train_ser, val_ser, test_ser","metadata":{"id":"VlkhQigKvOQT","execution":{"iopub.status.busy":"2022-07-13T15:42:11.765267Z","iopub.execute_input":"2022-07-13T15:42:11.765615Z","iopub.status.idle":"2022-07-13T15:42:11.874806Z","shell.execute_reply.started":"2022-07-13T15:42:11.765585Z","shell.execute_reply":"2022-07-13T15:42:11.873457Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model\n","metadata":{"id":"5vBLynXu0QI1"}},{"cell_type":"markdown","source":"## Positional encoding","metadata":{"id":"A9qUAZ00xiVd"}},{"cell_type":"markdown","source":"The formula for calculating the positional encoding is as follows:\n\n$${PE_{(pos, 2i)} = \\sin(pos / 10000^{2i / d_{model}})} $$\n$${PE_{(pos, 2i+1)} = \\cos(pos / 10000^{2i / d_{model}})} $$\n\nwhere $d_{model}$ is the model dimension, $pos$ is the position and $i$ is the index of the embedding.\nthis is taken from the paper: attention is all you need.","metadata":{"id":"_V1b_16-kp3v"}},{"cell_type":"code","source":"def create_positional_encoding(max_len: int, d_model: int) -> tf.Tensor:\n    \"\"\"Returns the positional encoding for a given a maximal sequence length and model dimension.\n    used in SeTransformer.__init__()\n    inputs: max_len: int, d_model: int\n    returns: tf.Tensor of shape (1, max_len, d_model) and dtype f_type\n    The 1 is for the batch dimension, the place in the batch dimension does not matter\"\"\"\n\n    def get_angles(positions: np.ndarray, timestamps: np.ndarray, d_model: int) -> np.ndarray:\n        \"\"\"Returns the angle in radians for given positions, timestamps and the dimension of the model\n        input: positions: np.ndarray of shape (max_len, 1), timestamps: np.ndarray of shape (1, d_model), d_model: int\n        output: np.ndarray of shape (max_len, d_model)\"\"\"\n        if f_type == \"float32\":\n            angle_rates = 1 / np.power(10000, ((2 * (timestamps//2)) / np.float32(d_model)))\n        else:\n            angle_rates = 1 / np.power(10000, ((2 * (timestamps//2)) / np.float16(d_model)))\n\n        return positions * angle_rates\n    \n    angle_rads = get_angles(np.arange(max_len)[:, np.newaxis],\n                            np.arange(d_model)[np.newaxis, :],\n                            d_model)  # (max_len, d_model)\n\n    # apply sin to even indices in the array; 2i for i in range(d_model // 2)\n    angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])  # (max_len, d_model)\n    # first dim: get all, second dim: start with index 0, jumps of 2\n\n    # apply cos to odd indices in the array; 2i+1\n    angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])  # (max_len, d_model)\n    # first dim: get all, second dim: start with index 1, jumps of 2\n\n    pos_encode = angle_rads[np.newaxis, ...]  # (1, max_len, d_model)\n\n    return tf.cast(pos_encode, dtype=f_type)","metadata":{"id":"Wx4hze4k0yls","execution":{"iopub.status.busy":"2022-07-13T15:42:11.879943Z","iopub.execute_input":"2022-07-13T15:42:11.880569Z","iopub.status.idle":"2022-07-13T15:42:11.888769Z","shell.execute_reply.started":"2022-07-13T15:42:11.880529Z","shell.execute_reply":"2022-07-13T15:42:11.887972Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Masking","metadata":{"id":"6ib5F3hnxrE9"}},{"cell_type":"markdown","source":"Mask all the pad tokens in the batch of sequence. It ensures that the model does not treat padding as the input. The mask indicates where pad value 0 is present: it outputs a 1 at those locations, and a 0 otherwise.","metadata":{"id":"3TXw70UlzcVi"}},{"cell_type":"markdown","source":"The look-ahead mask is used to mask the future tokens in a sequence. In other words, the mask indicates which entries should not be used.\n\nThis means that to predict the third token, only the first and second token will be used. Similarly to predict the fourth token, only the first, second and the third tokens will be used and so on.","metadata":{"id":"4w68l26GzrqG"}},{"cell_type":"code","source":"def create_masks(inp: tf.Tensor, tar: tf.Tensor, pad_ten: tf.Tensor) -> Tuple[tf.Tensor, tf.Tensor]:\n        \"\"\"Creates all the masks needed for the model\n        input: inp: tf.Tensor of shape (batch_size, seq_len), tar: tf.Tensor of shape (batch_size, set_size)\n        Returns: tuple of (padding_mask, look_ahead_mask)\n        padding_mask, look_ahead_mask: tf.Tensor of shape (batch_size, 1, 1, seq_len)\"\"\"\n        \n        def create_padding_mask(seq: tf.Tensor) -> tf.Tensor:\n                \"\"\"Returns a padding mask for the given sequence.\n                input: seq: tf.Tensor of shape (batch_size, seq_len)\n                Returns: tf.Tensor of shape (batch_size, 1, 1, seq_len)\"\"\"\n                seq = tf.cast(tf.math.equal(seq, pad_ten), f_type)  \n                # For every item in the sequence, 1 if it is a padding token, 0 if it is not \n\n                # add extra dimensions to add the padding\n                \n                return seq[:, tf.newaxis, tf.newaxis, :]  # (batch_size, 1, 1, seq_len)\n        \n        # Encoder padding mask\n        padding_mask: tf.Tensor = create_padding_mask(inp)  # (batch_size, 1, 1, seq_len)\n\n        # Used in the 1st attention block in the decoder.\n        # It is used to pad and mask future tokens in the input received by\n        # the decoder.\n        set_size: int = tar.shape[1]\n\n        def create_look_ahead_mask(set_size: int) -> tf.Tensor:\n                mask = 1 - tf.linalg.band_part(tf.ones((set_size, set_size)), -1, 0)\n                mask = tf.cast(mask, dtype=f_type)\n                return mask  # (seq_len, seq_len)\n\n        look_ahead_mask = create_look_ahead_mask(set_size)  # (seq_len, seq_len)\n        dec_target_padding_mask = create_padding_mask(tar)  # (batch_size, 1, 1, seq_len)\n        look_ahead_mask = tf.maximum(dec_target_padding_mask, look_ahead_mask) # (batch_size, 1, 1, seq_len)\n\n        return padding_mask, look_ahead_mask","metadata":{"id":"BQkjgUVVckzJ","execution":{"iopub.status.busy":"2022-07-13T15:42:11.890756Z","iopub.execute_input":"2022-07-13T15:42:11.891689Z","iopub.status.idle":"2022-07-13T15:42:11.907953Z","shell.execute_reply.started":"2022-07-13T15:42:11.891642Z","shell.execute_reply":"2022-07-13T15:42:11.907013Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Layers and blocks","metadata":{"id":"9KHKaxz3xumc"}},{"cell_type":"code","source":"class ScaledDotProductAttention(layers.Layer):\n    def __init__(self, d_model: int, **kwargs):\n        super(ScaledDotProductAttention, self).__init__(**kwargs)\n        # scale = 1 / sqrt(d_model)\n        self.scale = tf.math.pow(tf.cast(d_model, f_type), -0.5)\n        self.softmax = layers.Softmax(axis=-1)\n\n    def call(self, q: tf.Tensor, k: tf.Tensor, v: tf.Tensor, mask: Optional[tf.Tensor] = None) -> tf.Tensor:\n        \"\"\"Scaled Dot-Product Attention\n        input: \n        q: tf.Tensor of shape (batch_size, seq_len, d_model), \n        k: tf.Tensor of shape (batch_size, seq_len, d_model), \n        v: tf.Tensor of shape (batch_size, seq_len, d_model), \n        mask: Optional[tf.Tensor] of shape (batch_size, 1, 1, seq_len)\n        output: tf.Tensor of shape (batch_size, seq_len, d_model)\"\"\"\n        matmul_qk: tf.Tensor = tf.matmul(q, k, transpose_b=True)  # (..., seq_len_q, seq_len_k)\n        # q @ transpose(k)\n\n        # Scaled Dot-Product Attention\n        scaled_attention_logits: tf.Tensor = matmul_qk * self.scale  # (..., seq_len_q, seq_len_k)\n        # matmul_qk / sqrt(d_model)\n\n        # Masking\n        if mask is not None:\n            # noinspection PyTypeChecker\n            if f_type == \"float16\":\n                # tf.float16.min is minus infinity\n                scaled_attention_logits += (mask * tf.float16.min)  # changed from -1e9 to prevent nan's\n            else:\n                scaled_attention_logits += (mask * -1e9) \n\n        # Normalize\n        attention_weights = self.softmax(scaled_attention_logits)\n        # (..., seq_len_q, seq_len_k)\n\n        # Output\n        output = tf.matmul(attention_weights, v)\n\n        return output","metadata":{"id":"gRXst-o2NjXL","execution":{"iopub.status.busy":"2022-07-13T15:42:11.909634Z","iopub.execute_input":"2022-07-13T15:42:11.910288Z","iopub.status.idle":"2022-07-13T15:42:11.926798Z","shell.execute_reply.started":"2022-07-13T15:42:11.910250Z","shell.execute_reply":"2022-07-13T15:42:11.925727Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class MyMultiHeadAttention(Model):\n    \"\"\"U can use the built-in layers.multihead_attention but is caused a bug for me\"\"\"\n    def __init__(self, num_heads: int, d_model: int, **kwargs):\n        super(MyMultiHeadAttention, self).__init__(**kwargs)\n        if d_model % num_heads != 0:\n            raise ValueError(f\"d_model ({d_model}) must be divisible by num_heads ({num_heads})\")\n        self.num_heads = num_heads\n        self.d_model = d_model\n        self.depth = d_model // self.num_heads\n\n        self.wq = layers.Dense(d_model)\n        self.wk = layers.Dense(d_model)\n        self.wv = layers.Dense(d_model)\n\n        self.dense = layers.Dense(d_model)\n        self.sdpa = ScaledDotProductAttention(d_model)\n\n        \n    def split_heads(self, x: tf.Tensor, batch_size: int) -> tf.Tensor:\n        \"\"\"Split the last dimension into (num_heads, depth).\n        Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n        \"\"\"\n        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n        return tf.transpose(x, perm=[0, 2, 1, 3])\n\n    \n    def call(self, v_k: tf.Tensor, q: tf.Tensor, mask: tf.Tensor) -> tf.Tensor:\n        \"\"\"inputs:\n        v_k: tf.Tensor of shape (batch_size, seq_len, d_model) in self attention keys and values are the same\n        q: tf.Tensor of shape (batch_size, seq_len, d_model)\n        mask: Optional[tf.Tensor] of shape (batch_size, seq_len)\"\"\"\n        batch_size = tf.shape(q)[0]\n\n        q: tf.Tensor = self.wq(q)  # (batch_size, seq_len, d_model)\n        k: tf.Tensor = self.wk(v_k)  # (batch_size, seq_len, d_model)\n        v: tf.Tensor = self.wv(v_k)  # (batch_size, seq_len, d_model)\n\n        q: tf.Tensor = self.split_heads(q, batch_size)  # (batch_size, num_heads, seq_len_q, depth)\n        k: tf.Tensor = self.split_heads(k, batch_size)  # (batch_size, num_heads, seq_len_k, depth)\n        v: tf.Tensor = self.split_heads(v, batch_size)  # (batch_size, num_heads, seq_len_v, depth)\n\n        # scaled_attention.shape should be (batch_size, num_heads, seq_len_q, depth)\n        scaled_attention = self.sdpa(q, k, v, mask)\n\n        scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3]) \n         # (batch_size, seq_len_q, num_heads, depth)\n\n        concat_attention = tf.reshape(scaled_attention, (batch_size, -1, self.d_model))\n          # (batch_size, seq_len_q, d_model)\n\n        output = self.dense(concat_attention)  # (batch_size, seq_len_q, d_model)\n\n        return output","metadata":{"id":"_0I9SZI0kp31","pycharm":{"name":"#%%\n"},"execution":{"iopub.status.busy":"2022-07-13T15:42:11.928522Z","iopub.execute_input":"2022-07-13T15:42:11.929217Z","iopub.status.idle":"2022-07-13T15:42:11.944812Z","shell.execute_reply.started":"2022-07-13T15:42:11.929178Z","shell.execute_reply":"2022-07-13T15:42:11.943750Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class PointWiseFeedForward(Model):\n    def __init__(self, d_model: int, dff: int, **kwargs): \n        super(PointWiseFeedForward, self).__init__(**kwargs)\n        self.layer1 = layers.Dense(dff, activation=\"relu\")  # (batch_size, seq_len, dff)\n        self.layer2 = layers.Dense(d_model)  # (batch_size, seq_len, d_model)\n    \n    def call(self, x: tf.Tensor) -> tf.Tensor:\n        \"\"\"Gets and returns tensor of shape (batch_size, seq_len, d_model) and dtype keras.beckend.floatx()\"\"\"\n        x = self.layer1(x)\n        x = self.layer2(x)\n        return x","metadata":{"id":"QOVbpMxtNjXQ","execution":{"iopub.status.busy":"2022-07-13T15:42:11.946455Z","iopub.execute_input":"2022-07-13T15:42:11.947167Z","iopub.status.idle":"2022-07-13T15:42:11.960174Z","shell.execute_reply.started":"2022-07-13T15:42:11.947128Z","shell.execute_reply":"2022-07-13T15:42:11.959092Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class EncoderBlock(Model):\n    def __init__(self, d_model: int, num_heads: int, dff: int, drop_out_rate: float, **kwargs):\n        super(EncoderBlock, self).__init__(**kwargs)\n\n        self.mha = MyMultiHeadAttention(num_heads = num_heads, d_model = d_model)\n        self.ffn = PointWiseFeedForward(d_model, dff)\n\n        self.layer_norm = layers.LayerNormalization(epsilon=1e-6)\n\n        self.dropout = layers.Dropout(drop_out_rate)\n\n    def call(self, x: tf.Tensor, training: bool, mask: tf.Tensor) -> tf.Tensor:\n        \n        attn_output = self.mha(x, x, mask)  # (batch_size, input_seq_len, d_model)\n        attn_output = self.dropout(attn_output, training=training)  # (batch_size, input_seq_len, d_model)\n        # out1 = self.layer_norm(x + attn_output)  # (batch_size, input_seq_len, d_model)\n        # might be data leak\n        out1 = self.layer_norm(attn_output)  # (batch_size, input_seq_len, d_model)\n        \n        ffn_output = self.ffn(out1)  # (batch_size, input_seq_len, d_model)\n        ffn_output = self.dropout(ffn_output, training=training)  # (batch_size, input_seq_len, d_model)\n        out2 = self.layer_norm(out1 + ffn_output)  # (batch_size, input_seq_len, d_model)\n\n        return out2","metadata":{"id":"8mEP3-Jx2n39","execution":{"iopub.status.busy":"2022-07-13T15:42:11.961913Z","iopub.execute_input":"2022-07-13T15:42:11.962586Z","iopub.status.idle":"2022-07-13T15:42:11.973280Z","shell.execute_reply.started":"2022-07-13T15:42:11.962545Z","shell.execute_reply":"2022-07-13T15:42:11.972115Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class DecoderBlock(Model):\n    def __init__(self, d_model: int, num_heads: int, dff: int, rate: float, **kwargs):\n        super(DecoderBlock, self).__init__(**kwargs)\n\n        self.mha = MyMultiHeadAttention(num_heads = num_heads, d_model = d_model)\n\n        self.ffn = PointWiseFeedForward(d_model, dff)\n\n        self.layer_norm = layers.LayerNormalization(epsilon=1e-6)\n\n        self.dropout = layers.Dropout(rate)\n\n    def call(self, x: tf.Tensor, enc_output: tf.Tensor, look_ahead_mask: tf.Tensor, padding_mask: tf.Tensor, training):\n        # enc_output.shape should be (batch_size, input_seq_len, d_model)\n\n        attn1 = self.mha(x, x, look_ahead_mask)  # (batch_size, set_size, d_model)\n        attn1 = self.dropout(attn1, training=training)  # (batch_size, set_size, d_model)\n        # out1 = self.layer_norm(attn1 + x)\n        # might be data leak\n        out1 = self.layer_norm(attn1)  # (batch_size, set_size, d_model)\n\n        attn2 = self.mha(enc_output, out1, padding_mask)  # (batch_size, set_size, d_model)\n        attn2 = self.dropout(attn2, training=training)  # (batch_size, set_size, d_model)\n        out2 = self.layer_norm(attn2 + out1)  # (batch_size, set_size, d_model)\n\n        ffn_output = self.ffn(out2)  # (batch_size, set_size, d_model)\n        ffn_output = self.dropout(ffn_output, training=training)\n        out3 = self.layer_norm(ffn_output + out2)  # (batch_size, set_size, d_model)\n\n        return out3 ","metadata":{"id":"xJWFrv0g281G","execution":{"iopub.status.busy":"2022-07-13T15:42:11.974819Z","iopub.execute_input":"2022-07-13T15:42:11.975275Z","iopub.status.idle":"2022-07-13T15:42:11.991541Z","shell.execute_reply.started":"2022-07-13T15:42:11.975237Z","shell.execute_reply":"2022-07-13T15:42:11.990674Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Encoder(Model):\n    def __init__(self, pos_encoding: tf.Tensor, num_blocks: int, d_model: int, num_heads: int, dff: int, rate=0.1, **kwargs):\n        super(Encoder, self).__init__(**kwargs)\n\n        self.d_model = d_model\n        self.num_blocks = num_blocks\n        self.pos_encoding = pos_encoding\n\n        self.enc_blocks = [EncoderBlock(d_model, num_heads, dff, rate) for _ in range(num_blocks)]\n        # the encoder \n        self.dropout = layers.Dropout(rate)\n        self.scale = tf.math.sqrt(tf.cast(self.d_model, f_type))\n\n    def call(self, x: tf.Tensor, training, mask: tf.Tensor) -> tf.Tensor:\n\n        seq_len = tf.shape(x)[1]\n\n        # adding position encoding.\n        # assert not tf.math.is_nan(x[0][0][0])\n        x *= self.scale\n        # assert not tf.math.is_nan(x[0][0][0])\n        \n        x += self.pos_encoding[:, :seq_len, :]  # (batch_size, input_seq_len, d_model)\n        # assert not tf.math.is_nan(x[0][0][0])\n        x = self.dropout(x, training=training)  # (batch_size, input_seq_len, d_model)\n        # assert not tf.math.is_nan(x[0][0][0])\n\n        for block in self.enc_blocks:\n            x = block(x, training, mask)  # (batch_size, input_seq_len, d_model)\n            # assert not tf.math.is_nan(x[0][0][0])\n\n        return x  # (batch_size, input_seq_len, d_model)  ","metadata":{"id":"NdlS1kfM3k5d","execution":{"iopub.status.busy":"2022-07-13T15:42:11.993021Z","iopub.execute_input":"2022-07-13T15:42:11.994146Z","iopub.status.idle":"2022-07-13T15:42:12.008496Z","shell.execute_reply.started":"2022-07-13T15:42:11.994105Z","shell.execute_reply":"2022-07-13T15:42:12.007595Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Decoder(Model):\n    def __init__(self, pos_encoding, num_blocks: int, d_model: int, num_heads: int, dff: int,\n                 vocab_size: int, rate: float, **kwargs):\n        super(Decoder, self).__init__(**kwargs)\n\n        self.scale = tf.math.sqrt(tf.cast(d_model, f_type))\n        self.num_blocks = num_blocks\n        self.pos_encoding = pos_encoding\n\n        self.embedding = layers.Embedding(vocab_size, d_model)\n        self.dec_blocks = [DecoderBlock(d_model, num_heads, dff, rate) for _ in range(num_blocks)]\n        self.dropout = layers.Dropout(rate)\n\n    def call(self, tar: tf.Tensor, enc_output: tf.Tensor, training: bool,\n             look_ahead_mask: tf.Tensor, padding_mask: tf.Tensor) -> tf.Tensor:\n\n        seq_len = tf.shape(tar)[1]\n\n        x = self.embedding(tar)  # (batch_size, set_size, d_model)\n        x *= self.scale\n        x += self.pos_encoding[:, :seq_len, :]\n\n        x = self.dropout(x, training=training)\n\n        for block in self.dec_blocks:\n            x = block(x=x, enc_output=enc_output, look_ahead_mask=look_ahead_mask,\n                      padding_mask=padding_mask, training=training)\n\n        # x.shape should be (batch_size, set_size, d_model)\n        return x","metadata":{"id":"QLE8js6O3p5-","execution":{"iopub.status.busy":"2022-07-13T15:42:12.009780Z","iopub.execute_input":"2022-07-13T15:42:12.010706Z","iopub.status.idle":"2022-07-13T15:42:12.025492Z","shell.execute_reply.started":"2022-07-13T15:42:12.010668Z","shell.execute_reply":"2022-07-13T15:42:12.024250Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class EmbeddingTransposed(layers.Layer):\n    def __init__(self, tied_to: layers.Embedding = None, activation: Optional[str] = None, **kwargs):\n        super(EmbeddingTransposed, self).__init__(trainable=tied_to._trainable, **kwargs)\n        self.tied_to = tied_to\n        self.activation = keras.activations.get(activation)\n\n    def build(self, input_shape):\n        self.custom_weights = self.tied_to.weights[0]\n        self.built = True\n\n    def compute_output_shape(self, input_shape):\n        return input_shape[0], keras.backend.int_shape(self.tied_to.weights[0])[0]\n\n    def call(self, inputs, mask=None):\n        output = keras.backend.dot(inputs, keras.backend.transpose(self.custom_weights))\n        if self.activation is not None:\n            output = self.activation(output)\n        return output\n\n    def get_config(self):\n        config = {\"activation\": keras.activations.serialize(self.activation)}\n        base_config = super(EmbeddingTransposed, self).get_config()\n        return dict(list(base_config.items()) + list(config.items()))","metadata":{"id":"6uaYfyRdEtBN","execution":{"iopub.status.busy":"2022-07-13T15:42:12.026989Z","iopub.execute_input":"2022-07-13T15:42:12.027754Z","iopub.status.idle":"2022-07-13T15:42:12.042603Z","shell.execute_reply.started":"2022-07-13T15:42:12.027714Z","shell.execute_reply":"2022-07-13T15:42:12.041411Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## The full model","metadata":{"id":"1-xe3K7LyD2l"}},{"cell_type":"code","source":"def count_layers(my_model) -> int:\n    \"\"\"Counts the layers of a keras model recursizely\"\"\"\n    if not isinstance(my_model, keras.Model): \n        if isinstance(my_model, layers.Layer):\n            return 1\n        return 0\n    return sum([count_layers(sub_model) for sub_model in my_model.layers])","metadata":{"execution":{"iopub.status.busy":"2022-07-13T15:42:12.044091Z","iopub.execute_input":"2022-07-13T15:42:12.044904Z","iopub.status.idle":"2022-07-13T15:42:12.060266Z","shell.execute_reply.started":"2022-07-13T15:42:12.044860Z","shell.execute_reply":"2022-07-13T15:42:12.059112Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def line_prepender(_from: str, to: str, add: str):\n    with open(_from, 'r') as f:\n        content = f.read()\n    with open(to, \"w\") as f:\n        f.write(add + '\\n' + content)","metadata":{"execution":{"iopub.status.busy":"2022-07-13T15:42:12.061819Z","iopub.execute_input":"2022-07-13T15:42:12.062390Z","iopub.status.idle":"2022-07-13T15:42:12.071477Z","shell.execute_reply.started":"2022-07-13T15:42:12.062356Z","shell.execute_reply":"2022-07-13T15:42:12.070210Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if device == \"kaggle\":\n    glove_txt_path = \"../input/glove6b/glove.6B.300d.txt\"\nelse:\n    !wget http://nlp.stanford.edu/data/glove.6B.300d.zip\n    !apt install unzip\n    !unzip \"glove.6B.300d.zip\"\n    glove_txt_path = \"glove.6B.300d.txt\"\npro_path = \"./edited_glove.txt\"\nout_put_path = \"./gensim_glove_vectors.txt\"\n\nglove2word2vec(glove_input_file=glove_txt_path, word2vec_output_file=out_put_path)\nline_prepender(glove_txt_path, pro_path, \"400000 300\")\nkeyed_vectors = KeyedVectors.load_word2vec_format(pro_path, binary=False)\nglove_embedding = keyed_vectors.get_keras_embedding()","metadata":{"execution":{"iopub.status.busy":"2022-07-13T15:42:12.744411Z","iopub.execute_input":"2022-07-13T15:42:12.745142Z","iopub.status.idle":"2022-07-13T15:43:43.608292Z","shell.execute_reply.started":"2022-07-13T15:42:12.745103Z","shell.execute_reply":"2022-07-13T15:43:43.606808Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class SeTransformer(Model):\n    \"\"\"The base architecture of my models in this project.\"\"\"\n    def __init__(self, num_blocks: int, d_model: int, num_heads: int, dff: int,\n                 vocab_size: int, max_len: int, rate: float, pad_int: int, **kwargs):\n        super(SeTransformer, self).__init__(**kwargs)  # calls keras.Model's __init__ method with kwarg as key worg arguments\n        self.pad_int = pad_int\n        pos_encoding = create_positional_encoding(max_len, d_model)\n        self.encoder = Encoder(pos_encoding, num_blocks, d_model, num_heads, dff, rate)\n        self.decoder = Decoder(pos_encoding, num_blocks, d_model, num_heads, dff, vocab_size, rate)\n        self.embedding = glove_embedding\n        self.emb_trans = EmbeddingTransposed(self.embedding, \"softmax\")\n\n\n    def get_layer_count(self) -> int:\n        return count_layers(self)\n    \n    \n    def summary(self, **kwargs) -> None:\n        super().summary(**kwargs)\n        print(f\"The model have {self.get_layer_count()} layers\")\n        \n    \n    def count_params(self) -> int:\n        \"\"\"counts trainable parameters\n        Raises an error if caleed before building the model\"\"\"\n        param_count: int = self.encoder.count_params() + self.decoder.count_params() + self.embedding.count_params()\n        return param_count\n    \n    \n    def build_graph(self) -> keras.Model:\n        \"\"\"Returns a functional keras model identical to the model\"\"\"\n        inp = layers.Input(shape=(batch_size, max_seq_len))\n        tar = layers.Input(shape=(batch_size, set_size))\n        return keras.Model(inputs=[[inp, tar], True], outputs=self.call([inp, tar], True))\n    \n    \n    def call(self, inputs: List[tf.Tensor], training: bool) -> tf.Tensor:\n        inp, tar = inputs\n        # inp.shape should be (batch_size, max_seq_len)\n        # tar.shape should be (batch_size, set_size)\n        x = self.embedding(inp)  # (batch_size, max_seq_len, d_model)\n        padding_mask, look_ahead_mask = create_masks(inp, tar, self.pad_int)\n        enc_output = self.encoder(x, training, padding_mask)  # (batch_size, max_seq_len, d_model)\n        dec_output = self.decoder(tar, enc_output, training, look_ahead_mask, padding_mask)  # (batch_size, set_size, d_model)\n        final_output = self.emb_trans(dec_output)  # (batch_size, set_size, vocab_size)\n        return final_output","metadata":{"id":"4NLhyE0T3tUs","execution":{"iopub.status.busy":"2022-07-13T15:43:43.610544Z","iopub.execute_input":"2022-07-13T15:43:43.610955Z","iopub.status.idle":"2022-07-13T15:43:43.624064Z","shell.execute_reply.started":"2022-07-13T15:43:43.610919Z","shell.execute_reply":"2022-07-13T15:43:43.622939Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Training the model","metadata":{"id":"hFxiRfCekp38"}},{"cell_type":"code","source":"# model_art = wandb.use_artifact(f\"{model_collection_name}:latest\")\n# model_path = model_art.get_path(\"model.pb\").download()\n# model = tf.saved_model.load(model_path)","metadata":{"execution":{"iopub.status.busy":"2022-07-13T15:43:43.626179Z","iopub.execute_input":"2022-07-13T15:43:43.626933Z","iopub.status.idle":"2022-07-13T15:43:43.643928Z","shell.execute_reply.started":"2022-07-13T15:43:43.626888Z","shell.execute_reply":"2022-07-13T15:43:43.642562Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Hyper-Parameters","metadata":{"id":"B5rtigmHckyv"}},{"cell_type":"code","source":"set_size: int = 2\nlearning_rate: float = 0.005\n\nnum_sets: int = (max_seq_len // set_size) - 1 # Because we dont predict the first set\n# number of sets in each sequence\n\nnum_blocks: int = 16\nd_model: int = 300\ndff: int = 1024\nnum_heads: int = 30\ndropout_rate: float = 0.1","metadata":{"id":"bWmh89nVckyw","pycharm":{"name":"#%%\n"},"execution":{"iopub.status.busy":"2022-07-13T15:43:43.646889Z","iopub.execute_input":"2022-07-13T15:43:43.647362Z","iopub.status.idle":"2022-07-13T15:43:43.656098Z","shell.execute_reply.started":"2022-07-13T15:43:43.647328Z","shell.execute_reply":"2022-07-13T15:43:43.654917Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Weights and Biases","metadata":{}},{"cell_type":"code","source":" if using_tpu:\n    if device == \"kaggle\":\n        try:\n            from kaggle_secrets import UserSecretsClient\n            user_secrets = UserSecretsClient()\n            os.environ[\"WANDB_API_KEY\"] = user_secrets.get_secret(\"WANDB_API_KEY\")\n        except Exception:\n            print(\"please enter your weights and biases API key\")\n    !wandb login","metadata":{"execution":{"iopub.status.busy":"2022-07-13T15:43:43.657555Z","iopub.execute_input":"2022-07-13T15:43:43.657975Z","iopub.status.idle":"2022-07-13T15:43:43.668920Z","shell.execute_reply.started":"2022-07-13T15:43:43.657934Z","shell.execute_reply":"2022-07-13T15:43:43.667588Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# try: \n#     artifect = use_artifact(artifact, use_as=None)\n#     art = wandb.use_artifact(...)\n#     wandb.run.link_artifact(art, \"yonikremer/final_project_owned/version0\")","metadata":{"execution":{"iopub.status.busy":"2022-07-13T15:43:43.670521Z","iopub.execute_input":"2022-07-13T15:43:43.670945Z","iopub.status.idle":"2022-07-13T15:43:43.682446Z","shell.execute_reply.started":"2022-07-13T15:43:43.670909Z","shell.execute_reply":"2022-07-13T15:43:43.681355Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if not \"run\" in globals() and using_tpu:\n    run = wandb.init(\n        project=\"final_project_owned\",\n        entity=\"yonikremer\",\n        name=datetime.datetime.today().strftime(\"run from %d/%m/%Y\"),\n        settings=wandb.Settings(start_method=\"thread\"),\n        config = {\"set size\": set_size,\n                  \"batch size\": batch_size,\n                  \"learning rate\": learning_rate,\n                  \"max seq len\": max_seq_len,\n                  \"num blocks\": num_blocks,\n                  \"model dimention\": d_model,\n                  \"dff\": dff,\n                  \"num heads\": num_heads,\n                  \"dropout rate\": dropout_rate\n                  })\n    config = wandb.config","metadata":{"execution":{"iopub.status.busy":"2022-07-13T15:43:43.684647Z","iopub.execute_input":"2022-07-13T15:43:43.685601Z","iopub.status.idle":"2022-07-13T15:43:43.695281Z","shell.execute_reply.started":"2022-07-13T15:43:43.685542Z","shell.execute_reply":"2022-07-13T15:43:43.694293Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Create the model","metadata":{"id":"O783T1xMckzR"}},{"cell_type":"code","source":"model = SeTransformer(\n    num_blocks=num_blocks,\n    d_model=d_model,\n    num_heads=num_heads,\n    dff=dff,\n    vocab_size=vocab_size,\n    max_len=max_seq_len,\n    rate=dropout_rate,\n    pad_int=pad_int)\n\noptimizer = tf.keras.optimizers.Adam(learning_rate, epsilon=keras.backend.epsilon())\nloss_func = keras.losses.SparseCategoricalCrossentropy(from_logits=False)\nacc = keras.metrics.SparseCategoricalAccuracy(dtype=f_type)\n\nmodel.compile(optimizer=optimizer,\n    loss=loss_func,\n    metrics=[acc]\n )","metadata":{"execution":{"iopub.status.busy":"2022-07-13T15:43:43.696923Z","iopub.execute_input":"2022-07-13T15:43:43.697963Z","iopub.status.idle":"2022-07-13T15:43:44.768574Z","shell.execute_reply.started":"2022-07-13T15:43:43.697899Z","shell.execute_reply":"2022-07-13T15:43:44.767481Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"temp_input = tf.random.uniform((batch_size, max_seq_len), dtype=tf.int32, minval=1, maxval=vocab_size-1)\ntemp_target = tf.random.uniform((batch_size, set_size), dtype=tf.int32, minval=1, maxval=vocab_size-4)\ntemp_target2 = temp_target + 3\n\ntrain_pred1 = model([temp_input, temp_target], training=False)\ntrain_pred2 = model([temp_input, temp_target], training=False)\ntrain_pred3 = model([temp_input, temp_target2], training=False)\n\ntry:\n    tf.debugging.assert_equal(train_pred1, train_pred2)\n    print(\"the model is determenistic\")\n    try:\n        tf.debugging.assert_equal(train_pred1, train_pred3)\n    except tf.errors.InvalidArgumentError:\n        print(\"WARNING: model output might depends on the target\")\nexcept tf.errors.InvalidArgumentError:\n    print(\"The model is not determenistic and have sone random noise\")\nparam_count: int = model.count_params()\nprint(f\"The model has {param_count:,} = {round(param_count * (10**-6), 1)}M trainable parameters\")\nif using_tpu: run.config[\"parameters\"] = param_count\n\n# stats = FlopCoKeras(model)\n# flops_per_call: int = stats.total_flops\n# macs_per_call: int = stats.total_macs\n\n# # (add-multiplies per forward pass) * (2 FLOPs/add-multiply) * (3 for forward and backward pass) * (number of examples in dataset) \n# training_flops: float  = macs_per_call * 2 * flops_per_call / macs_per_call * (3 * train_step_calles + val_step_calles)\n# print(f\"FLOPs per call: {flops_per_call:,} = {(flops_per_call * (10 ** -6)):,}M\")\n# print(f\"MACs per call: {macs_per_call:,} = {(macs_per_call * (10 ** -6)):,}M\")\ndel temp_input, temp_target, temp_target2, train_pred1, train_pred2, train_pred3","metadata":{"id":"-uAxXGBtkp4A","pycharm":{"name":"#%%\n"},"outputId":"898c89bb-118b-4c84-e398-4171a2904b42","execution":{"iopub.status.busy":"2022-07-13T15:48:57.211938Z","iopub.execute_input":"2022-07-13T15:48:57.213536Z","iopub.status.idle":"2022-07-13T15:49:01.942242Z","shell.execute_reply.started":"2022-07-13T15:48:57.213479Z","shell.execute_reply":"2022-07-13T15:49:01.940636Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# keras.utils.plot_model(\n#     model.build_graph(),\n#     show_shapes=True,\n#     show_dtype=True,\n#     show_layer_names=False,\n#     expand_nested=True,\n#     layer_range=None,\n#     show_layer_activations=True\n# )","metadata":{"execution":{"iopub.status.busy":"2022-07-13T15:43:46.084265Z","iopub.status.idle":"2022-07-13T15:43:46.085162Z","shell.execute_reply.started":"2022-07-13T15:43:46.084928Z","shell.execute_reply":"2022-07-13T15:43:46.084955Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# model.summary(line_length=125, positions=[0.5, 0.66, 0.83, 1], expand_nested=True)","metadata":{"execution":{"iopub.status.busy":"2022-07-13T15:43:46.086699Z","iopub.status.idle":"2022-07-13T15:43:46.087158Z","shell.execute_reply.started":"2022-07-13T15:43:46.086954Z","shell.execute_reply":"2022-07-13T15:43:46.086976Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"inputs = [layers.Inputs(shape=[None,], dtype=tf.int32), layers.Inputs(shape=[set_size,], dtype=tf.int32)]\nx = layers.Embedding(16, set_size)(inputs[0])\n\n# Conv1D + global max pooling\nx = layers.Conv1D(128, 7, padding=\"valid\", activation=\"relu\", strides=3)(x)\nx = layers.Conv1D(128, 7, padding=\"valid\", activation=\"relu\", strides=3)(x)\n\n# We add a vanilla hidden layer:\nx = layers.Dense(128, activation=\"relu\")(x)\n\n# We project onto a single unit output layer, and squash it with a sigmoid:\npredictions = layers.Dense(1, activation=\"sigmoid\", name=\"predictions\")(x)\n\nmodel = tf.keras.Model(inputs, predictions)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Training helper functions","metadata":{"id":"5gjCRldwckzR"}},{"cell_type":"code","source":"@tf.function(input_signature=([tf.TensorSpec(shape=[2, 5], dtype=tf.int32)]))\ndef index_start(batch):\n    \"\"\"gets a two d tensor and returns the index of the first column than contains a non pad_int value\n    example: [[pad_ten, not_pad_ten, not_pad_ten], \n              [pad_ten, pad_ten, not_pad_ten]] -> 1\"\"\"\n    ans = batch.shape[1]\n    for i in range(batch.shape[1]):\n        print(tf.not_equal(batch[:, i], pad_ten))\n        cond = tf.reduce_any(tf.not_equal(batch[:, i], pad_ten))\n        print(cond)\n        print(tf.get_static_value(cond))\n        print(cond.numpy())\n        if tf.get_static_value(cond):\n            ans = tf.math.minimum(i, ans)\n            print(ans)\n    return ans","metadata":{"id":"HZf9y_1nCtEa","execution":{"iopub.status.busy":"2022-07-13T17:33:46.041804Z","iopub.execute_input":"2022-07-13T17:33:46.042933Z","iopub.status.idle":"2022-07-13T17:33:46.059423Z","shell.execute_reply.started":"2022-07-13T17:33:46.042885Z","shell.execute_reply":"2022-07-13T17:33:46.058364Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"index_start(tf.Variable([[0, 0, 0, 1, 8], [0, 0, -3, -1, 8]]))","metadata":{"execution":{"iopub.status.busy":"2022-07-13T17:33:49.206722Z","iopub.execute_input":"2022-07-13T17:33:49.207499Z","iopub.status.idle":"2022-07-13T17:33:49.386294Z","shell.execute_reply.started":"2022-07-13T17:33:49.207448Z","shell.execute_reply":"2022-07-13T17:33:49.384504Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Train","metadata":{"id":"iRP0mGA1NjXi"}},{"cell_type":"code","source":"@tf.function(input_signature=(tf.TensorSpec(shape=[batch_size, None], dtype=tf.int32),\n                              tf.TensorSpec(shape=[batch_size, set_size], dtype=tf.int32)))\ndef train_step(inp: tf.Tensor, outp: tf.Tensor) -> tf.Tensor:\n    with tf.GradientTape() as tape:\n#         pred: tf.Tensor = model([inp, outp], training=True) \n        pred: tf.Tensor = model([inp, outp])\n        loss_val: tf.Tensor = loss_func(y_true = outp, y_pred = pred)\n    grads: tf.RaggedTensor = tape.gradient(loss_val, model.trainable_weights)\n    optimizer.apply_gradients(zip(grads, model.trainable_weights))\n    return tf.math.reduce_mean(loss_val), acc(outp, pred)","metadata":{"id":"UR6qYltHhh_t","execution":{"iopub.status.busy":"2022-07-13T16:07:43.275202Z","iopub.execute_input":"2022-07-13T16:07:43.275623Z","iopub.status.idle":"2022-07-13T16:07:43.785231Z","shell.execute_reply.started":"2022-07-13T16:07:43.275590Z","shell.execute_reply":"2022-07-13T16:07:43.784028Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"@tf.function(input_signature=[tf.TensorSpec(shape=[batch_size, max_seq_len], dtype=tf.int32)])\ndef train(batch: tf.Tensor) -> tf.TensorSpec(shape=[], dtype=f_type):\n    per_gen_loss: tf.Tensor = tf.zeros([num_sets], dtype=f_type)\n    per_gen_acc: tf.Tensor = tf.zeros([num_sets], dtype=f_type)\n    i = 0\n    while i < num_sets:\n        # The input is of size set_size-TAKE_TO_ACCOUNT\n        already_predicted: int = i * (set_size + 1)\n        start_from: int = max(0, already_predicted - max_seq_len)\n        inp: tf.Tensor = batch[:, start_from:(i + 1) * set_size]\n        have_pad = tf.map_fn(contains_pad, inp, fn_output_signature=tf.bool, parallel_iterations=batch_size)\n        if tf.get_static_value(tf.math.reduce_all(have_pad)):\n            break\n        outp: tf.TensorSpec(shape=[batch_size, set_size]) = batch[:, (i + 1) * set_size:(i + 2) * set_size]\n        loss_val, acc_val = train_step(inp, outp)\n        one_hot_loss = tf.one_hot([i], num_sets, dtype = f_type) * loss_val\n        one_hot_acc = tf.one_hot([i], num_sets, dtype = f_type) * acc_val\n        per_gen_loss += one_hot_loss\n        per_gen_acc += one_hot_acc\n        i += 1\n    return tf.math.reduce_mean(per_gen_loss[:i]), tf.math.reduce_mean(per_gen_acc[:i])\n    ","metadata":{"id":"sSQ8nWVwMuG_","execution":{"iopub.status.busy":"2022-07-13T16:07:47.592275Z","iopub.execute_input":"2022-07-13T16:07:47.592741Z","iopub.status.idle":"2022-07-13T16:07:47.603810Z","shell.execute_reply.started":"2022-07-13T16:07:47.592704Z","shell.execute_reply":"2022-07-13T16:07:47.602924Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\ntrain(tf.random.uniform(shape=[batch_size, max_seq_len], minval=0, maxval=vocab_size-1, dtype=tf.int32))","metadata":{"execution":{"iopub.status.busy":"2022-07-13T16:07:52.559746Z","iopub.execute_input":"2022-07-13T16:07:52.560420Z","iopub.status.idle":"2022-07-13T16:07:52.719227Z","shell.execute_reply.started":"2022-07-13T16:07:52.560375Z","shell.execute_reply":"2022-07-13T16:07:52.718053Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Validate","metadata":{"id":"N9vu221UNjXk"}},{"cell_type":"code","source":"@tf.function(input_signature=(tf.TensorSpec(shape=[batch_size, None], dtype=tf.int32),\n                              tf.TensorSpec(shape=[batch_size, set_size], dtype=tf.int32)))\ndef val_step(inp: tf.Tensor, outp: tf.Tensor) -> tf.Tensor:\n    pred = model([inp, outp], training=False)\n    loss_val = loss_func(y_true = outp, y_pred = pred)\n    return tf.math.reduce_mean(loss_val), acc(outp, pred)","metadata":{"id":"gPYi_RT4Ytv_","execution":{"iopub.status.busy":"2022-07-13T16:05:30.072066Z","iopub.execute_input":"2022-07-13T16:05:30.073256Z","iopub.status.idle":"2022-07-13T16:05:30.080065Z","shell.execute_reply.started":"2022-07-13T16:05:30.073209Z","shell.execute_reply":"2022-07-13T16:05:30.078754Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"@tf.function(input_signature=[tf.TensorSpec(shape=[batch_size, max_seq_len], dtype=tf.int32)])\ndef validate(batch: tf.Tensor) -> tf.TensorSpec(shape=[], dtype=f_type):\n    per_generation_loss: tf.Tensor = tf.zeros([num_sets], dtype=f_type)\n    per_gen_acc: tf.Tensor = tf.zeros([num_sets], dtype=f_type)\n    i = 0\n    while i < num_sets:\n        # The input is of size set_size-TAKE_TO_ACCOUNT\n        already_predicted: int = i * (set_size + 1)\n        start_from: int = max(0, already_predicted - max_seq_len)\n        inp: tf.Tensor = batch[:, start_from:(i + 1) * set_size]\n        have_pad = tf.map_fn(contains_pad, inp, fn_output_signature=tf.bool, parallel_iterations=batch_size)\n        if tf.get_static_value(tf.math.reduce_all(have_pad)):\n            break\n        loss_val, acc_val = val_step(inp, outp)\n        one_hot_loss = tf.one_hot([i], num_sets, dtype=f_type) * loss_val\n        one_hot_acc = tf.one_hot([i], num_sets, dtype=f_type) * acc_val\n        per_gen_loss += one_hot_loss\n        per_gen_acc += one_hot_acc\n        i += 1\n    return tf.math.reduce_mean(per_gen_loss[:i]), tf.math.reduce_mean(per_gen_acc[:i])","metadata":{"id":"7NpdkcUkckzT","pycharm":{"name":"#%%\n"},"execution":{"iopub.status.busy":"2022-07-13T16:05:25.611966Z","iopub.execute_input":"2022-07-13T16:05:25.612404Z","iopub.status.idle":"2022-07-13T16:05:25.628096Z","shell.execute_reply.started":"2022-07-13T16:05:25.612367Z","shell.execute_reply":"2022-07-13T16:05:25.627171Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\nvalidate(tf.random.uniform(shape=[batch_size, max_seq_len], minval=0, maxval=vocab_size-1, dtype=tf.int32))","metadata":{"execution":{"iopub.status.busy":"2022-07-13T16:05:33.350292Z","iopub.execute_input":"2022-07-13T16:05:33.350685Z","iopub.status.idle":"2022-07-13T16:05:33.668146Z","shell.execute_reply.started":"2022-07-13T16:05:33.350643Z","shell.execute_reply":"2022-07-13T16:05:33.666887Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Callbacks","metadata":{"id":"cEcysTey1bEk"}},{"cell_type":"code","source":"best_loss = float(\"inf\")\nbest_model = None","metadata":{"id":"Iuce_A94ckzU","pycharm":{"name":"#%%\n"},"execution":{"iopub.status.busy":"2022-07-13T15:43:46.102917Z","iopub.status.idle":"2022-07-13T15:43:46.103538Z","shell.execute_reply.started":"2022-07-13T15:43:46.103322Z","shell.execute_reply":"2022-07-13T15:43:46.103345Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def check_point(model: SeTransformer, train_loss, train_acc, val_loss, val_acc, test_loss = None, test_acc = None):\n    \"\"\"Saves the model at the end of each epoch\"\"\"\n    # (add-multiplies per forward pass) * (2 FLOPs/add-multiply) * \n    # * (3 for forward and backward pass) * (number of examples in dataset)\n    global best_model\n    global last_save_time\n    last_save_time = time.time()\n    best_model = model\n    num_ops: float  = macs_per_call * 2 * flops_per_call / macs_per_call * (3 * train_step_calles + val_step_calles)\n    keras.models.save_model(model = model, filepath = \"model.pb\", save_format=\"tf\", overwrite=True)\n    wandb.log({\"model train loss\": train_loss, \"model train acc\": train_acc, \"model val loss\": val_loss, \"model val acc\": val_acc, \n              \"model test loss\": test_loss, \"model test acc\": test_acc})\n    art = wandb.Artifact(f\"{wandb.run.id}-best model\", type=\"my_model\", description = f\"the model after {num_ops:,} operations\")\n    art.add_file(\"model.pb\")\n    run.log_artifact(artifact)\n    print(\"Saved checkpoint\")","metadata":{"id":"LZsEGz_vckzT","pycharm":{"name":"#%%\n"},"execution":{"iopub.status.busy":"2022-07-13T15:43:46.104786Z","iopub.status.idle":"2022-07-13T15:43:46.105177Z","shell.execute_reply.started":"2022-07-13T15:43:46.105007Z","shell.execute_reply":"2022-07-13T15:43:46.105025Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def on_val_batch_end(train_loss: float, train_acc: float ,val_loss: float, val_acc: float) -> bool:\n    \"\"\"A callback after every val batch\n    returns True if the model should stop training and False else\"\"\"\n    global best_loss\n    global best_model\n    global last_save_time\n    if time.time() - last_save_time > 1800.0 and val_loss < math.log(vocab_size) and val_loss < best_loss:\n        best_loss = val_loss\n        # If the last save is more than a half hour (1800 sec) ago\n        # and if the predictions are better than randon and \n        check_point(model, train_loss, train_acc, val_loss, val_acc)\n        return False\n    elif train_loss < 0.01:\n        title: str = \"Over fitting or data leak\"\n        message = f\"Training loss is {train_loss} and val loss is {val_loss} in the latest batch\"\n        wandb.alert(title=title, text=message)\n        print(title)\n        print(message)\n        return True\n    elif time.time() - last_save_time > 18000.0 and train_loss >= math.log(vocab_size):\n        # if the prob of every token is 1/vocab_size, the loss is\n        # -ln(1/vocab_size) = ln(vocab_size) \n        # by the logrithem rule log(a^x)=xlog(a) where x = -1\n        # if after 5 hours of training, the model predictions are still random\n        title: str = \"Under fitting\"\n        message = f\"train loss: {train_loss} train acc: {train_acc}, val loss: {val_loss}, val acc: {val_acc} in the latest batch\"\n        wandb.alert(title=title, text=message)\n        print(title)\n        print(message)\n        return True\n    return Flase","metadata":{"execution":{"iopub.status.busy":"2022-07-13T15:43:46.106716Z","iopub.status.idle":"2022-07-13T15:43:46.107139Z","shell.execute_reply.started":"2022-07-13T15:43:46.106957Z","shell.execute_reply":"2022-07-13T15:43:46.106976Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## The actual training loop!","metadata":{"id":"RN8rmbrNW0-Z"}},{"cell_type":"code","source":"def train_loop():\n    epochs: int = 1000000  # Train until the cloud disconnects or the model stops improving\n    per_epoch_train_loss: List[float] = []\n    per_epoch_val_loss: List[float] = []\n    per_epoch_train_acc: List[float] = []\n    per_epoch_val_acc: List[float] = []\n    print(f\"number of train batches per epoch: {len(list_train_set)}\")\n    last_save_time = time.time()\n    for epoch in range(epochs):\n        print(f\"epoch number: {epoch}\")\n        per_batch_train_loss: List[float] = []\n        per_batch_val_loss: List[float] = []\n        per_batch_train_acc: List[float] = []\n        per_batch_val_acc: List[float] = []\n        for batch_num in tqdm.tqdm(range(len(list_train_set))):  # tqdm is a progress bar\n            train_loss, train_acc = train(list_train_set[batch_num])\n            float_train_loss = keras.backend.eval(train_loss).item()\n            per_batch_train_loss.append(float_train_loss)\n            float_train_acc = keras.backend.eval(train_acc).item()\n            per_batch_train_acc.append(float_train_acc)\n            if using_tpu:\n                wandb.log({\"epoch\": epoch, \"batch\": batch_num, \"batch train loss\": float_train_loss, \n                           \"batch train_acc\": float_train_acc})\n            if batch_num % 8 == 0:  # 8 = #training batches/#val batches\n                # because training set is 80% of the data and val set is 10%\n                next_val_batch: tf.Tensor = list_val_set[batch_num // 8]\n                val_loss, val_acc = validate(next_val_batch)\n                float_val_loss = keras.backend.eval(val_loss).item()\n                per_batch_val_loss.append(float_val_loss)\n                float_val_acc = keras.backend.eval(val_acc).item()\n                per_batch_val_acc.append(float_val_acc)\n                if using_tpu:\n                    wandb.log({\"epoch\": epoch, \"batch\": batch_num, \"batch val loss\": float_val_loss,\n                               \"batch train_acc\": float_val_acc})\n                    on_val_batch_end(float_train_loss, float_train_acc, float_val_loss, float_val_acc)\n        epoch_train_loss = statistics.mean(per_batch_train_loss)\n        epoch_train_acc = statistics.mean(per_batch_train_acc)\n        epoch_val_loss = statistics.mean(per_batch_val_loss)\n        epoch_val_acc = statistics.mean(per_batch_val_acc)\n        per_epoch_train_loss.append(epoch_train_loss)\n        per_epoch_train_acc.append(epoch_train_acc)\n        per_epoch_val_loss.append(epoch_val_loss)\n        per_epoch_val_acc.append(epoch_val_acc)\n        print(f\"train loss: {epoch_train_loss}\")\n        print(f\"train acc: {epoch_train_acc}\")\n        print(f\"val loss: {epoch_val_loss}\")\n        print(f\"val acc: {epoch_val_acc}\")\n        if len(per_epoch_val_loss) > 1:\n            if epoch_val_loss >= per_epoch_val_loss[-2]:\n                print(\"Validation loss increased. Stopped training\")\n                return epoch_train_loss, epoch_val_loss, epoch_train_acc, epoch_val_acc\n        print(\"Saved checkpoint\")","metadata":{"execution":{"iopub.status.busy":"2022-07-13T15:43:46.108120Z","iopub.status.idle":"2022-07-13T15:43:46.108470Z","shell.execute_reply.started":"2022-07-13T15:43:46.108304Z","shell.execute_reply":"2022-07-13T15:43:46.108321Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!export AUTOGRAPH_VERBOSITY=10\nif using_tpu:\n    with strategy.scope():\n        train_loss, val_loss, train_acc, val_acc = train_loop()\nelse:\n    train_loss, val_loss, train_acc, val_acc = train_loop()","metadata":{"id":"g5MukK3ynG7v","outputId":"dca87c06-0140-41d9-c5bd-1e71ea9c9289","execution":{"iopub.status.busy":"2022-07-13T15:43:46.109885Z","iopub.status.idle":"2022-07-13T15:43:46.110220Z","shell.execute_reply.started":"2022-07-13T15:43:46.110048Z","shell.execute_reply":"2022-07-13T15:43:46.110064Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## After training","metadata":{"id":"Z-iopedNNjXs"}},{"cell_type":"code","source":"if best_model and using_tpu:\n    global model\n    model = tf.lite.TFLiteConverter.from_saved_model(\"model.pb\")\n    test_loss, test_acc = statistics.mean([validate(test_batch) for test_batch in tqdm.tqdm(list_test_set)])\n    print(f\"Test loss: {test_loss}\")\n    print(f\"Test acc: {test_acc}\")\n    wandb.log({\"test loss\": test_loss, \"text acc\": test_acc})","metadata":{"id":"1VtC8sP_NjXu","execution":{"iopub.status.busy":"2022-07-13T15:43:46.111156Z","iopub.status.idle":"2022-07-13T15:43:46.111499Z","shell.execute_reply.started":"2022-07-13T15:43:46.111332Z","shell.execute_reply":"2022-07-13T15:43:46.111348Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}