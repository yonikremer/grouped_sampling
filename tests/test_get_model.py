# Generated by CodiumAI
import os
import random

import numpy as np
import pytest
import torch
from torch._dynamo import OptimizedModule
from transformers import AutoTokenizer

from src.grouped_sampling.model import get_model

"""
Code Analysis

Objective:
The objective of the 'get_model' function is to load a pre-trained model from the Hugging Face Transformers library, with the option to use 8-bit quantization and CUDA acceleration, and return the compiled model.

Inputs:
- model_name (str): the name or path of the pre-trained model to load
- load_in_8bit (bool): whether to use 8-bit quantization for the model (default False)
- use_cuda (bool): whether to use CUDA acceleration (default True)
- **kwargs: additional keyword arguments to pass to the 'from_pretrained' method of the 'AutoModelForCausalLM' class

Flow:
1. Determine if 8-bit quantization and CUDA acceleration should be used based on the input parameters.
2. Create a dictionary of keyword arguments to pass to the 'from_pretrained' method of the 'AutoModelForCausalLM' class, including the model name/path, 8-bit quantization flag, resume download flag, and any additional keyword arguments passed in.
3. If 8-bit quantization is being used, set the 'device_map' key in the dictionary to 'auto'.
4. Load the pre-trained model using the 'from_pretrained' method of the 'AutoModelForCausalLM' class with the created dictionary of keyword arguments.
5. If CUDA acceleration is being used and available, move the model to the GPU.
6. If the model has an 'eval' method, call it.
7. Compile the model using the 'compile' method of the 'torch' module.
8. Return the compiled model.

Outputs:
- model (PreTrainedModel): the loaded and compiled pre-trained model

Additional aspects:
- If the model is too large for the GPU and CUDA acceleration is being used, a warning is issued and the model is moved to the CPU.
- If CUDA acceleration is not available, a warning is issued and the model is loaded on the CPU.
"""


class TestGetModel:
    #  Tests that the function returns a PreTrainedModel object
    def test_returns_model_object(self):
        model = get_model('gpt2')
        assert isinstance(model, OptimizedModule)

    #  Tests that the function loads the model with the correct kwargs
    def test_loads_correct_kwargs(self):
        model_name = 'gpt2'
        kwargs = {'output_hidden_states': True}
        model = get_model(model_name, **kwargs)
        assert model.config.output_hidden_states == kwargs['output_hidden_states']

    #  Tests that the function warns when CUDA is not available
    def test_warns_when_cuda_not_available(self):
        with pytest.warns(UserWarning):
            model = get_model('gpt2', use_cuda=False)

    #  Tests that the function loads the model in 8bit correctly
    def test_loads_model_in_8bit(self):
        model_name = 'gpt2'
        get_model(model_name, load_in_8bit=True)

    # Tests that the model gives the same output when getting the same input
    def test_model_gives_same_output(self):
        model_name = 'gpt2'
        model = get_model(model_name)
        tokenizer = AutoTokenizer.from_pretrained(model_name)
        input_ids = tokenizer('Hello, my dog is cute', return_tensors='pt')["input_ids"]
        input_ids = input_ids.cuda()
        output1 = model(input_ids)
        logits1 = output1.logits
        output2 = model(input_ids)
        logits2 = output2.logits
        assert torch.equal(logits1, logits2)

    # Tests that the model gives the same output whith and without batching
    def test_model_gives_same_output_with_and_without_batching(self):
        torch.manual_seed(0)
        random.seed(0)
        np.random.seed(0)
        os.environ['TOKENIZERS_PARALLELISM'] = 'false'
        model_name = 'fxmarty/tiny-llama-fast-tokenizer'
        model = get_model(model_name)
        tokenizer = AutoTokenizer.from_pretrained(model_name)
        prompts = ['Hello, my dog is cute'] * 2
        input_ids_batch = tokenizer(prompts, return_tensors='pt')["input_ids"].cuda()
        with torch.no_grad():
            output_batch = model(input_ids_batch)
        logits_batch = output_batch.logits

        input_ids = tokenizer(prompts[0], return_tensors='pt')["input_ids"].cuda()
        with torch.no_grad():
            output = model(input_ids)
        logits = output.logits
        assert torch.equal(logits_batch[0], logits_batch[1])
        assert logits_batch.shape == (2, input_ids.shape[1], model.config.vocab_size)
        assert logits.shape == (1, input_ids.shape[1], model.config.vocab_size)
        assert logits.dtype == logits_batch.dtype
        for i in range(input_ids.shape[1]):
            assert torch.isclose(logits_batch[0][i], logits[0][i], atol=1e-5, rtol=1e-4).all(),\
                f'Index {i} does not match. got {logits_batch[0][i]} and {logits[0][i]}'
