
# Generated by CodiumAI

import pytest
import torch
from torch import Tensor, long

from src.grouped_sampling.end_to_end_pipeline import EndToEndSingleSequencePipeLine

"""
Code Analysis

Main functionalities:
The EndToEndSingleSequencePipeLine class is responsible for generating text based on a given prompt using a pre-trained language model. It takes a model name and a boolean flag indicating whether to load the model in 8-bit mode as input. The class has three main functionalities: converting a string prompt to a LongTensor of tokens, converting a LongTensor of tokens to a string, and generating a string of text of a specified length based on a given prompt.

Methods:
- string_to_tokens: converts a string prompt to a LongTensor of tokens using the tokenizer associated with the model.
- tokens_to_logit_matrix: generates a logits matrix of shape (output_length, vocab_size) based on a given sequence of tokens and a target output length. The logits are not divided by the temperature in this function.
- tokens_to_string: converts a LongTensor of tokens to a string using the tokenizer associated with the model.
- __call__: generates a string of text of a specified length based on a given prompt. It first converts the prompt to a sequence of tokens using string_to_tokens, then generates a logits matrix using tokens_to_logit_matrix, and finally generates a string of text using tokens_to_string.

Fields:
- tokenizer: the tokenizer associated with the model.
- model: the pre-trained language model.
- device: the device on which the model is loaded.
- max_input_len: the maximum length of the input sequence that the model can handle.
- logit_vector_to_token: an instance of the LogitVectorToTokenPipeLine class, which is responsible for converting logits vectors to tokens.
"""


def string_to_tokens(
        pipeline,
        string: str,
) -> Tensor:
    """Convert a string to a Tensor of tokens."""
    tokens = torch.tensor(
        pipeline.tokenizer.encode(string),
        device=pipeline.device,
        dtype=long,
        requires_grad=False,
    )
    return tokens


class TestEndToEndSingleSequencePipeLine:
    #  Tests that the pipeline returns an empty string when given an empty prompt and output_length of 0
    def test_output_length_0(self):
        pipeline = EndToEndSingleSequencePipeLine('gpt2')
        assert pipeline.generate('This is an example', 0) == ''

    #  Tests that the pipeline raises an exception when given a prompt and output_length greater than the max_position_embeddings
    def test_output_length_greater_than_max_position_embeddings(self):
        pipeline = EndToEndSingleSequencePipeLine('gpt2')
        prompt = 'This is a prompt.'
        output_length = pipeline.max_total_len + 1
        with pytest.raises(ValueError):
            pipeline.generate(prompt, output_length)

    #  Tests that the pipeline raises an exception when given a prompt and output_length equal to the max_position_embeddings
    def test_output_length_equal_to_max_position_embeddings(self):
        pipeline = EndToEndSingleSequencePipeLine('gpt2')
        prompt = 'This is a prompt.'
        output_length = pipeline.max_total_len
        with pytest.raises(ValueError):
            pipeline.generate(prompt, output_length)

    #  Tests that the pipeline returns a string when given a prompt and output_length less than the length of the prompt
    def test_output_length_less_than_prompt_length(self):
        pipeline = EndToEndSingleSequencePipeLine('gpt2')
        prompt = 'This is a prompt.'
        output_length = 2
        result = pipeline.generate(prompt, output_length)
        assert isinstance(result, str)
        assert len(result) >= output_length

    #  Tests that the pipeline raises a TypeError when given a non-string prompt
    def test_non_string_prompt(self):
        pipeline = EndToEndSingleSequencePipeLine('gpt2')
        with pytest.raises(TypeError):  # noinspection PyTypeChecker
            pipeline.generate(123, 10)

    #  Tests that the pipeline raises a ValueError when given an empty prompt
    def test_empty_prompt(self):
        pipeline = EndToEndSingleSequencePipeLine('gpt2')
        with pytest.raises(ValueError):
            pipeline.generate('', 10)

    def test_string_to_tokens_valid_prompt(self):
        pipeline = EndToEndSingleSequencePipeLine('gpt2')
        multi_token_prompt = 'This is a prompt.'
        tokens = string_to_tokens(pipeline, multi_token_prompt)
        assert isinstance(tokens, Tensor)
        assert tokens.dtype == long
        assert tokens.shape[0] > 1

    def test_string_to_tokens_one_token_prompt(self):
        pipeline = EndToEndSingleSequencePipeLine('gpt2')
        one_token_prompt = 'the'
        tokens = string_to_tokens(pipeline, one_token_prompt)
        assert isinstance(tokens, Tensor)
        assert tokens.dtype == long
        assert tokens.shape == (1, )

    def test_tokens_to_logit_matrix_valid_prompt(self):
        pipeline = EndToEndSingleSequencePipeLine('gpt2')
        prompt = 'This is a prompt.'
        tokens = string_to_tokens(pipeline, prompt)
        output_length = 10
        logits = pipeline.tokens_to_logit_matrix(tokens, output_length)
        assert isinstance(logits, Tensor)
        assert logits.dtype in (torch.float32, torch.float64)
        assert logits.shape == (output_length, pipeline.tokenizer.vocab_size)

    def test_tokens_to_logit_matrix_non_tensor_prompt(self):
        pipeline = EndToEndSingleSequencePipeLine('gpt2')
        tokens = 123
        output_length = 10
        with pytest.raises(TypeError):  # noinspection PyTypeChecker
            pipeline.tokens_to_logit_matrix(tokens, output_length)

    def test_tokens_to_logit_matrix_valid_prompt_output_length_less_than_prompt_length(self):
        pipeline = EndToEndSingleSequencePipeLine('gpt2')
        prompt = 'This is a prompt.'
        tokens = string_to_tokens(pipeline, prompt)
        output_length = 2
        logits = pipeline.tokens_to_logit_matrix(tokens, output_length)
        assert isinstance(logits, Tensor)
        assert logits.dtype in (torch.float32, torch.float64)
        assert logits.shape == (output_length, pipeline.tokenizer.vocab_size)

    def test_tokens_to_logit_matrix_valid_prompt_output_length_equal_to_prompt_length(self):
        pipeline = EndToEndSingleSequencePipeLine('gpt2')
        prompt = 'This is a prompt.'
        tokens = string_to_tokens(pipeline, prompt)
        output_length = len(tokens)
        logits = pipeline.tokens_to_logit_matrix(tokens, output_length)
        assert isinstance(logits, Tensor)
        assert logits.dtype in (torch.float32, torch.float64)
        assert logits.shape == (output_length, pipeline.tokenizer.vocab_size)

    def test_tokens_to_logit_matrix_valid_prompt_output_length_greater_than_prompt_length(self):
        pipeline = EndToEndSingleSequencePipeLine('gpt2')
        prompt = 'This is a prompt.'
        tokens = string_to_tokens(pipeline, prompt)
        output_length = len(tokens) + 1
        logits = pipeline.tokens_to_logit_matrix(tokens, output_length)
        assert isinstance(logits, Tensor)
        assert logits.dtype in (torch.float32, torch.float64)
        assert logits.shape == (output_length, pipeline.tokenizer.vocab_size)
