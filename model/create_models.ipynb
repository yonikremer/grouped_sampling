{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZZf-fbj50Kqc"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "device: str\n",
        "if 'google.colab' in sys.modules:\n",
        "    device = 'colab'\n",
        "if 'kaggle_web_client' in sys.modules:\n",
        "    device = 'kaggle'\n",
        "else:\n",
        "    device = 'locally'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PBAoO1bCUGLm"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "if device == 'colab':\n",
        "    %pip install -q tensorflow_text\n",
        "    %pip install -q tqdm\n",
        "    %pip install -q wandb --upgrade\n",
        "elif device == 'kaggle':\n",
        "    %pip install -q google-cloud-bigquery-storage\n",
        "    %pip install -q numpy==1.19.0\n",
        "    %pip install -q tensorflow==2.9.1\n",
        "    %pip install -q absl-py==0.9\n",
        "    %pip install -q matplotlib==3.1.1\n",
        "    %pip install -q protobuf==3.11.2\n",
        "    %pip install -q tensorflow_text==2.9.0\n",
        "    %pip install -q tqdm\n",
        "    %pip install -q wandb --upgrade\n",
        "\n",
        "%pip install flopco-keras"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "gather": {
          "logged": 1644854036589
        },
        "id": "MsfMr-Qod_nl",
        "outputId": "e09897f8-e611-4818-b6b6-adf1e370cccf"
      },
      "outputs": [
        {
          "data": {
            "application/javascript": "if (!(\"Notification\" in window)) {\n    alert(\"This browser does not support desktop notifications, so the %%notify magic will not work.\");\n} else if (Notification.permission !== 'granted' && Notification.permission !== 'denied') {\n    Notification.requestPermission(function (permission) {\n        if(!('permission' in Notification)) {\n            Notification.permission = permission;\n        }\n    })\n}\n",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# standard liberties:\n",
        "from typing import Optional, List, Set, Dict, Tuple\n",
        "import datetime\n",
        "import os\n",
        "import random\n",
        "import statistics\n",
        "import math\n",
        "import time\n",
        "import sys\n",
        "# NOT-standard liberties:\n",
        "import wandb\n",
        "import tensorflow as tf\n",
        "import tensorflow_text as tf_text\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "# My code:\n",
        "from setransformer import SeTransformer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "gather": {
          "logged": 1644854037100
        },
        "id": "SzaglEQ-kp2n",
        "outputId": "5b7fd3b9-3369-431c-b516-bde95519988f",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Python version: 3.7.13 (default, Apr 24 2022, 01:04:09) \n",
            "[GCC 7.5.0]\n",
            "Tensorflow version: 2.9.1\n",
            "tf text version: 2.9.0\n"
          ]
        }
      ],
      "source": [
        "print(f\"Python version: {sys.version}\")\n",
        "print(f\"Tensorflow version: {tf.__version__}\")\n",
        "print(f\"tf text version: {tf_text.__version__}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rJGjWnfTc6DI",
        "outputId": "c7472bec-3c99-4222-eba6-229027c5b6c6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "GPU info:\n",
            "Thu Jul  7 13:15:46 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   40C    P8     9W /  70W |      0MiB / 15109MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "print('GPU info:')\n",
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "91ZJ0-RZIx6j"
      },
      "source": [
        "# Settings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1644854038983
        },
        "id": "2mp2yFTEkp2u",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "tf.random.set_seed(0)\n",
        "random.seed(0)\n",
        "# tf.keras.backend.set_floatx('float16')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QlubPo7LRyNm"
      },
      "source": [
        "## Define a strategy - Accelerator optimization "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s0yqg12zqSt9"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "# disable printing\n",
        "try:\n",
        "    resolver = tf.distribute.cluster_resolver.TPUClusterResolver();\n",
        "    tf.config.experimental_connect_to_cluster(resolver);\n",
        "    tf.tpu.experimental.initialize_tpu_system(resolver);\n",
        "    strategy = tf.distribute.TPUStrategy(resolver);\n",
        "    using_tpu: bool = True;\n",
        "except ValueError:\n",
        "    print(\"You must connect to a TPU in order to train a model. The models dont fit in a colab GPU\")\n",
        "    exit()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yVR1oJRD0NtO"
      },
      "source": [
        "# Data loading"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "gather": {
          "logged": 1644857858611
        },
        "id": "pn1UfAXQwlQX",
        "outputId": "af20cbcb-3d71-4d4b-be9a-5850b9f9741c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /drive; to attempt to forcibly remount, call drive.mount(\"/drive\", force_remount=True).\n",
            "(30279, 2)\n"
          ]
        }
      ],
      "source": [
        "if device == 'colab':  # If notebook is ran on colab\n",
        "    from google.colab import drive\n",
        "    drive.mount('/drive')\n",
        "    df: pd.DataFrame = pd.read_csv('/drive/MyDrive/final_project/wikipedia_articles.csv')\n",
        "else:  # If notebook is ran on my laptop\n",
        "    df: pd.DataFrame = pd.read_csv('wiki_data/articles.csv')\n",
        "print(df.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IW43lG6iUV6G",
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "outputId": "3fad9123-c6f0-4cf6-f625-6d22d0241bda"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "There are 30279 data points\n",
            "The length of the longest text IN CHARACTERS is: 141803\n",
            "The length of the shortest text IN CHARACTERS is: 816\n"
          ]
        }
      ],
      "source": [
        "df: pd.Series = df['text']\n",
        "data_list: List[str] = df.to_list()\n",
        "DATA_SIZE = len(data_list)\n",
        "print(f\"There are {DATA_SIZE} data points\")\n",
        "string_lengths: List[int] = [len(data_point) for data_point in data_list]\n",
        "max_string_len = max(string_lengths)\n",
        "print(f\"The length of the longest text IN CHARACTERS is: {max_string_len}\")\n",
        "min_string_len = min(string_lengths)\n",
        "print(f\"The length of the shortest text IN CHARACTERS is: {min_string_len}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "87Putqz6kp20"
      },
      "source": [
        "## Creating the vocabulary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KBhC3SPyMi9n"
      },
      "outputs": [],
      "source": [
        "# %%time\n",
        "bert_tokenizer_params: dict = dict(lower_case=True)\n",
        "VOCAB_SIZE: int = 8192  # Always the same for all models\n",
        "\n",
        "if device == 'colab':  # If notebook is ran on colab\n",
        "    path = '/drive/MyDrive/final_project/vocab.txt'\n",
        "else:  # If notebook is ran on my laptop\n",
        "    path = 'C:/yoni/final_project/model/vocab.txt'\n",
        "\n",
        "\n",
        "reserved_tokens: List[str] = [\"[PAD]\", \"[UNK]\", \"[START]\", \"[END]\", \"[MASK]\"]\n",
        "\n",
        "if os.path.exists(path):\n",
        "    with open(path, 'r') as f:\n",
        "        vocab: List[str] = f.read().split()\n",
        "else:\n",
        "    vocab_args: dict = dict(\n",
        "        # The target vocabulary size\n",
        "        vocab_size = VOCAB_SIZE,\n",
        "        # Reserved tokens that must be included in the vocabulary\n",
        "        reserved_tokens=reserved_tokens,\n",
        "        # Arguments for `tf_text.BertTokenizer`\n",
        "        bert_tokenizer_params=bert_tokenizer_params,\n",
        "        # Arguments for `wordpiece_vocab.wordpiece_tokenizer_learner_lib.learn`\n",
        "        learn_params={},\n",
        "    )\n",
        "    tensor_list: list = [tf.convert_to_tensor(data_point) for data_point in data_list]\n",
        "    data_set: tf.data.Dataset = tf.data.Dataset.from_tensor_slices(tensor_list)\n",
        "    # I already ran this code and saved the file to C:/yoni/final_project/model/vocab.txt\n",
        "    vocab: List[str] = tf_text.bert_vocab_from_dataset.bert_vocab_from_dataset(\n",
        "        data_set,\n",
        "        **vocab_args,)\n",
        "    with open('C:/yoni/final_project/model/vocab.txt', 'w') as f:\n",
        "        for token in vocab:\n",
        "            f.write(token + ' ')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4IZ5uxlJEoi3",
        "outputId": "a6886ce0-ea09-47e8-8ad6-3cc9e0bb8d25"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "the type of the items in vocab: <class 'str'>\n",
            "the first 15 items in vocab: ['[PAD]', '[UNK]', '[START]', '[END]', '[MASK]', \"'\", ',', '.', '0', '1', '2', '3', '4', '5', '6']\n",
            " the length of vocab: 7882\n"
          ]
        }
      ],
      "source": [
        "print(f\"the type of the items in vocab: {type(vocab[0])}\")\n",
        "print(f\"the first 15 items in vocab: {vocab[:15]}\")\n",
        "print(f\" the length of vocab: {len(vocab)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hndA44nhFw6I",
        "outputId": "9d396370-3326-4700-8a31-8fc4fd4a8125"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " the type of the items in tensor_vocab is: <class 'tensorflow.python.framework.ops.EagerTensor'>\n",
            " the data type of the tensors in tensor_vocab is: <dtype: 'string'>\n"
          ]
        }
      ],
      "source": [
        "tensor_vocab: List[tf.Tensor] = [tf.convert_to_tensor(token_key, dtype=tf.string) for token_key in vocab]  # dtype = tf.String\n",
        "print(f\" the type of the items in tensor_vocab is: {type(tensor_vocab[0])}\")\n",
        "print(f\" the data type of the tensors in tensor_vocab is: {tensor_vocab[0].dtype}\")\n",
        "vocab_size = len(vocab)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "lD6QCFLAkp2_"
      },
      "source": [
        "## Creating the tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HihwIsrKD7C9"
      },
      "outputs": [],
      "source": [
        "lookup_table = tf.lookup.StaticVocabularyTable(\n",
        "    tf.lookup.KeyValueTensorInitializer(\n",
        "        keys=tensor_vocab,\n",
        "        key_dtype=tf.string,\n",
        "        values=tf.range(tf.size(vocab, out_type=tf.int64), dtype=tf.int64),\n",
        "        value_dtype=tf.int64),\n",
        "    num_oov_buckets=1\n",
        ")\n",
        "tokenizer = tf_text.BertTokenizer(lookup_table, **bert_tokenizer_params)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "A73Lbgg6kp3R"
      },
      "source": [
        "## Tokenizing the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mLxV5l562-x2"
      },
      "outputs": [],
      "source": [
        "# START: int = tf.argmax(tf.constant(reserved_tokens) == \"[START]\")  # The value of the start token\n",
        "# END: int = tf.argmax(tf.constant(reserved_tokens) == \"[END]\")  # The value of the end token\n",
        "# starts = tf.cast(tf.Variable([START]), dtype = tf.int32)  # Tensor of shape [1] and dtype int\n",
        "# ends = tf.cast(tf.Variable([END]), dtype = tf.int32)  # Tensor of shape [1] and dtype int\n",
        "starts = tf.constant([2], dtype=tf.int32)\n",
        "ends = tf.constant([3], dtype=tf.int32)\n",
        "pad_int: int = int(tf.argmax(tf.constant(reserved_tokens) == \"[PAD]\"))\n",
        "pad_ten: tf.TensorSpec(dtype=tf.int32, shape=()) = tf.constant([pad_int], dtype=tf.int32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4gPxVc7tkp3X",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "def tokenize_string(text: str) -> tf.Tensor:\n",
        "    \"\"\"Converts string to tensor\"\"\"\n",
        "    ragged: tf.RaggedTensor = tokenizer.tokenize(text)[0, :]\n",
        "    eager: tf.Tensor = ragged.to_tensor(default_value=0, shape=[None, 1])  # 0 is the value of the padding token\n",
        "    squeezed: tf.Tensor = tf.squeeze(eager, axis=1)\n",
        "    typed: tf.Tensor = tf.cast(squeezed, tf.int32)\n",
        "    edited: tf.Tensor = tf.concat([starts, typed, ends], axis=0)\n",
        "    return edited"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UzHmzHSukp3a",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "tokenized_data: List[tf.Tensor] = [tokenize_string(data_point) for data_point in data_list] \n",
        "\n",
        "# tqdm is a progress bar"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CNEdwxWFkp3d",
        "outputId": "2813d6de-6975-4cdb-8421-39479c7f02fb",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "30279\n",
            "(670,)\n",
            "tf.Tensor([   2 1011 7670   57   18 6423  617   33   61   44], shape=(10,), dtype=int32)\n"
          ]
        }
      ],
      "source": [
        "print(len(tokenized_data))\n",
        "print(tokenized_data[0].shape)\n",
        "print(tokenized_data[0][:10])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "EtjxgLWMkp3l"
      },
      "source": [
        "### chunk too long texts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MubdHBEMkJNC"
      },
      "outputs": [],
      "source": [
        "max_seq_len: int = 256\n",
        "def chunk_tensor(tensor: tf.Tensor, max_len: int = max_seq_len) -> List[tf.Tensor]:\n",
        "    \"\"\"Splits 1d tensor to chunks (1d tensors) of maximum size: max_len\"\"\"\n",
        "    return [tensor[i*max_len:(i+1)*max_len] for i in range(tensor.shape[0] // max_len)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9DgU2Or2kp3m",
        "outputId": "596803b2-a306-4ef7-92c0-a497ecc507e3",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "336056\n",
            "(256,)\n"
          ]
        }
      ],
      "source": [
        "chunked_data: List[tf.Tensor] = []\n",
        "for tensor in tokenized_data:\n",
        "    chunks = chunk_tensor(tensor, max_seq_len)\n",
        "    for chunk in chunks:\n",
        "        chunked_data.append(chunk)\n",
        "DATA_SIZE: int = len(chunked_data)\n",
        "print(DATA_SIZE)\n",
        "print(chunked_data[0].shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R8Z2_P2Lnke8"
      },
      "source": [
        "## Padding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XUOVoGBfkp3p",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "def pad(tensor: tf.Tensor, pad_int: int) -> tf.Tensor:\n",
        "    \"\"\"Pads the tensor to the length of the longest text in the data set\"\"\"\n",
        "    padded: tf.Tensor = tf.pad(tensor=tensor, paddings=[[pad_int, max_seq_len - tensor.shape[0]]], mode='CONSTANT', constant_values=0)\n",
        "    # 0 is the padding token\n",
        "    return padded"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nRa9_ryHkp3p",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "padded_data: List[tf.Tensor] = [pad(text, pad_int) for text in chunked_data]\n",
        "chunked_data.sort(key = lambda t: t.shape[0])  # sorting so that every batch will have similar sized texts"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "gbnub0UZkp3s"
      },
      "source": [
        "## Train test val split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pTW48b8Qr6sv"
      },
      "outputs": [],
      "source": [
        "batch_size: int = 128\n",
        "\n",
        "def list_to_dataset(tokenized_list: List[tf.Tensor]) -> tf.data.Dataset:\n",
        "    \"\"\"Converts a list of tokenized texts after all preprocessing to a tf.data.Dataset\"\"\"\n",
        "    dataset: tf.data.Dataset = tf.data.Dataset.from_tensor_slices(tokenized_list)\n",
        "    batched: tf.data.Dataset = dataset.batch(batch_size)\n",
        "    return batched\n",
        "\n",
        "batched_data_ten = list_to_dataset(padded_data)\n",
        "batched_data_list = list(batched_data_ten)\n",
        "random.shuffle(batched_data_list)\n",
        "if batched_data_list[-1].shape[0] != batch_size:  # if the last batch is smaller than batch_size\n",
        "    batched_data_list = batched_data_list[:-1]  # remove the last batch\n",
        "data_size = len(batched_data_list)\n",
        "train_size: int = int(data_size * 0.8) \n",
        "val_test_size: int = int(data_size * 0.1)  # Both validation and test get 10% of the data\n",
        "list_train_set: List[tf.Tensor] = batched_data_list[:train_size]\n",
        "list_val_set: List[tf.Tensor] = batched_data_list[train_size:(train_size + val_test_size)]\n",
        "list_test_set: List[tf.Tensor] = batched_data_list[(train_size + val_test_size)]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HzS72rcIvL0N"
      },
      "source": [
        "## Clear memory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VlkhQigKvOQT"
      },
      "outputs": [],
      "source": [
        "del batched_data_list, train_size, val_test_size, data_size\n",
        "del padded_data, chunked_data, tokenized_data, data_list, df\n",
        "del lookup_table, reserved_tokens\n",
        "del bert_tokenizer_params, ends, starts, vocab, tensor_vocab\n",
        "del chunk, chunks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "hFxiRfCekp38"
      },
      "source": [
        "# Training the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "B5rtigmHckyv"
      },
      "source": [
        "## Hyper-Parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bWmh89nVckyw",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "set_size: int = 2\n",
        "learning_rate: float = 0.01\n",
        "\n",
        "num_sets: int = (max_seq_len // set_size) - 1 # Because we dont predict the first set\n",
        "# number of sets in each sequence\n",
        "\n",
        "num_blocks: int = 8\n",
        "d_model: int = 256\n",
        "dff: int = 512\n",
        "num_heads: int = 16\n",
        "dropout_rate: float = 0.1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O783T1xMckzR"
      },
      "source": [
        "## Create the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-uAxXGBtkp4A",
        "outputId": "898c89bb-118b-4c84-e398-4171a2904b42",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The model has 4,084,480 trainable parameters\n"
          ]
        }
      ],
      "source": [
        "with strategy.scope():\n",
        "    model = SeTransformer(\n",
        "        num_blocks=num_blocks,\n",
        "        d_model=d_model,\n",
        "        num_heads=num_heads,\n",
        "        dff=dff,\n",
        "        vocab_size=vocab_size,\n",
        "        max_len=max_seq_len,\n",
        "        rate=dropout_rate,\n",
        "        pad_int=pad_int,\n",
        "        using_tpu=using_tpu)\n",
        "    \n",
        "    loss_func = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False)\n",
        "    optimizer = tf.keras.optimizers.Adam(learning_rate, epsilon=tf.keras.backend.epsilon())\n",
        "    temp_input = tf.random.uniform((batch_size, max_seq_len), dtype=tf.int32, minval=5, maxval=6999)\n",
        "    temp_target = tf.random.uniform((batch_size, set_size), dtype=tf.int32, minval=5, maxval=6999)\n",
        "    model.compile(optimizer=optimizer, loss=loss_func)\n",
        "\n",
        "param_count: int = model.count_params()\n",
        "print(f\"The model has {param_count:,} = {param_count * (10**-6):,}M trainable parameters\")\n",
        "stats = FlopCoKeras(model)\n",
        "flops_per_call: int = stats.total_flops\n",
        "macs_per_call: int = stats.total_macs\n",
        "\n",
        "# (add-multiplies per forward pass) * (2 FLOPs/add-multiply) * (3 for forward and backward pass) * (number of examples in dataset) \n",
        "training_flops: float  = macs_per_call * 2 * flops_per_call / macs_per_call * (3 * train_step_calls + val_step_calls)\n",
        "print(f\"FLOPs per call: {flops_per_call:,} = {(flops_per_call * (10 ** -6)):,}M\")\n",
        "print(f\"MACs per call: {macs_per_call:,} = {(macs_per_call * (10 ** -6)):,}M\")\n",
        "\n",
        "del temp_input, temp_target"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eRHYWRwmRmef"
      },
      "source": [
        "## Weights and Biases"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QK4HzQpGRxwY"
      },
      "outputs": [],
      "source": [
        "%wandb login\n",
        "# my API key is 58def12d67e682fb2c89ab27e91e612243568aba"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        },
        "id": "hSyTfD8FW01n",
        "outputId": "92fa8a12-07f2-48e6-fcdc-b0d1fc0f2f47"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33myonikremer\u001b[0m (\u001b[33myoniteam\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.12.21"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20220707_132821-5mn3jqy2</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href=\"https://wandb.ai/yoniteam/pytorch-intro/runs/5mn3jqy2\" target=\"_blank\">run from 07/07/2022</a></strong> to <a href=\"https://wandb.ai/yoniteam/pytorch-intro\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "run = wandb.init(\n",
        "    project=\"pytorch-intro\",\n",
        "    entity=\"yoniteam\",\n",
        "    name=datetime.datetime.today().strftime(f\"run from %d/%m/%Y\"),\n",
        "    settings=wandb.Settings(start_method=\"thread\"),\n",
        "    config = {\"set size\": set_size,\n",
        "              \"batch size\": batch_size,\n",
        "              \"learning rate\": learning_rate,\n",
        "              \"max seq len\": max_seq_len,\n",
        "              \"num blocks\": num_blocks,\n",
        "              \"model dimention\": d_model,\n",
        "              \"dff\": dff,\n",
        "              \"num heads\": num_heads,\n",
        "              \"dropout rate\": dropout_rate,\n",
        "              \"params\": param_count\n",
        "              })\n",
        "config = wandb.config"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5gjCRldwckzR"
      },
      "source": [
        "## Training helper functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HZf9y_1nCtEa"
      },
      "outputs": [],
      "source": [
        "@tf.function\n",
        "def contains_pad(inp: tf.Tensor):\n",
        "    bool_ten = tf.math.equal(inp, pad_ten)\n",
        "    nonzero_count = tf.math.count_nonzero(bool_ten)\n",
        "    return nonzero_count > 0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iRP0mGA1NjXi"
      },
      "source": [
        "### Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UR6qYltHhh_t"
      },
      "outputs": [],
      "source": [
        "@tf.function(input_signature=(tf.TensorSpec(shape=[batch_size, None], dtype=tf.int32),\n",
        "                              tf.TensorSpec(shape=[batch_size, set_size], dtype=tf.int32)))\n",
        "def train_step(inp: tf.Tensor, outp: tf.Tensor) -> tf.Tensor:\n",
        "    with tf.GradientTape() as tape:\n",
        "        pred: tf.Tensor = model([inp, outp], training=True) \n",
        "        loss_val: tf.Tensor = loss_func(y_true = outp, y_pred = pred)\n",
        "    grads: tf.RaggedTensor = tape.gradient(loss_val, model.trainable_weights)\n",
        "    optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
        "    return tf.math.reduce_mean(loss_val)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sSQ8nWVwMuG_"
      },
      "outputs": [],
      "source": [
        "@tf.autograph.experimental.do_not_convert\n",
        "@tf.function(input_signature=[tf.TensorSpec(shape=[batch_size, max_seq_len], dtype=tf.int32)])\n",
        "def train(batch: tf.Tensor) -> tf.TensorSpec(shape=[], dtype=tf.keras.backend.floatx()):\n",
        "    per_generation_loss: tf.Tensor = tf.zeros([num_sets], dtype=tf.keras.backend.floatx())\n",
        "    i = 0\n",
        "    while i < num_sets:\n",
        "        # The input is of size set_size-TAKE_TO_ACCOUNT\n",
        "        already_predicted: int = i * (set_size + 1)\n",
        "        start_from: int = max(0, already_predicted - max_seq_len)\n",
        "        inp: tf.Tensor = batch[:, start_from:(i + 1) * set_size]\n",
        "        have_pad = tf.map_fn(contains_pad, inp, fn_output_signature=tf.bool, parallel_iterations=batch_size)\n",
        "        if tf.get_static_value(tf.math.reduce_all(have_pad)):\n",
        "            break\n",
        "        outp: tf.TensorSpec(shape=[batch_size, set_size]) = batch[:, (i + 1) * set_size:(i + 2) * set_size]\n",
        "        loss_val: tf.TensorSpec(shape=[], dtype=tf.keras.backend.floatx()) = train_step(inp, outp)\n",
        "        one_hot: tf.TensorSpec(shape=[num_sets], dtype=tf.keras.backend.floatx())\n",
        "        one_hot = tf.one_hot([i], num_sets, dtype=tf.keras.backend.floatx()) * loss_val\n",
        "        per_generation_loss += one_hot\n",
        "        i += 1\n",
        "    train_step_calles += i \n",
        "    return tf.math.reduce_mean(per_generation_loss[:i])\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N9vu221UNjXk"
      },
      "source": [
        "### Validate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gPYi_RT4Ytv_"
      },
      "outputs": [],
      "source": [
        "@tf.function(input_signature=(tf.TensorSpec(shape=[batch_size, None], dtype=tf.int32),\n",
        "                                  tf.TensorSpec(shape=[batch_size, set_size], dtype=tf.int32)))\n",
        "def val_step(inp: tf.Tensor, outp: tf.Tensor) -> tf.Tensor:\n",
        "    pred = model([inp, outp], training=False)\n",
        "    loss_val = loss_func(y_true = outp, y_pred = pred)\n",
        "    return tf.math.reduce_mean(loss_val)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7NpdkcUkckzT",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "@tf.function(input_signature=[tf.TensorSpec(shape=[batch_size, max_seq_len], dtype=tf.int32)])\n",
        "def validate(batch: tf.Tensor) -> tf.TensorSpec(shape=[], dtype=tf.keras.backend.floatx()):\n",
        "    per_generation_loss: tf.Tensor = tf.zeros([num_sets], dtype=tf.keras.backend.floatx())\n",
        "    i = 0\n",
        "    while i < num_sets:\n",
        "        # The input is of size set_size-TAKE_TO_ACCOUNT\n",
        "        already_predicted: int = i * (set_size + 1)\n",
        "        start_from: int = max(0, already_predicted - max_seq_len)\n",
        "        inp: tf.Tensor = batch[:, start_from:(i + 1) * set_size]\n",
        "        have_pad = tf.map_fn(contains_pad, inp, fn_output_signature=tf.bool, parallel_iterations=batch_size)\n",
        "        if tf.get_static_value(tf.math.reduce_all(have_pad)):\n",
        "            break\n",
        "        outp: tf.TensorSpec(shape=[batch_size, set_size]) = batch[:, (i + 1) * set_size:(i + 2) * set_size]\n",
        "        loss_val: tf.TensorSpec(shape=[], dtype=tf.keras.backend.floatx()) = val_step(inp, outp)\n",
        "        one_hot: tf.TensorSpec(shape=[num_sets], dtype=tf.keras.backend.floatx())\n",
        "        one_hot = tf.one_hot([i], num_sets, dtype=tf.keras.backend.floatx()) * loss_val\n",
        "        per_generation_loss += one_hot\n",
        "        i += 1\n",
        "    val_step_calles += i\n",
        "    return tf.math.reduce_mean(per_generation_loss[:i])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cEcysTey1bEk"
      },
      "source": [
        "## Chackpoints"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Iuce_A94ckzU",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "date: str = datetime.datetime.now().strftime('%m%d-%H%M')\n",
        "if device == 'colab':\n",
        "    folder_path: str = \"/drive/MyDrive/final_project/checkpoints/\"\n",
        "else:\n",
        "    folder_path: str = \"C:/yoni/final_project/model/checkpoints/\"\n",
        "check_points_path = f\"{folder_path}{date}\"\n",
        "if not os.path.isdir(folder_path):\n",
        "    os.mkdir(folder_path)\n",
        "if not os.path.isdir(check_points_path):\n",
        "    os.mkdir(check_points_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LZsEGz_vckzT",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "def check_point(folder_path: str, model: SeTransformer, val_loss: float, train_loss: float, test_loss = None):\n",
        "    \"\"\"Saves the model at the end of each epoch\"\"\"\n",
        "    # (add-multiplies per forward pass) * (2 FLOPs/add-multiply) * \n",
        "    # * (3 for forward and backward pass) * (number of examples in dataset) \n",
        "    num_ops: float  = macs_per_call * 2 * flops_per_call / macs_per_call * (3 * train_step_calles + val_step_calles)\n",
        "    peta_ops: float = num_ops * (10 ** (-15))\n",
        "    tf.keras.models.save_model(model = model, filepath = folder_path, save_format='tf', overwrite=True)\n",
        "    artifact = wandb.Artifact('new_artifact', type='my_model', description = f\"the model after {num_ops:,} operations\")\n",
        "    artifact.add_dir(f'after_{training_flops:,}_ops/')\n",
        "    run.log_artifact(artifact)\n",
        "    print(\"Saved checkpoint\")\n",
        "    %notify(\"Saved checkpoint\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "00auA3LZwKvf"
      },
      "outputs": [],
      "source": [
        "train_loss, val_loss = float('inf'), float('inf')\n",
        "best_val_loss = float('inf')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VQENw4qSEssJ"
      },
      "outputs": [],
      "source": [
        "def loss_to_prob(loss: float) -> float:\n",
        "    return math.exp(-loss)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RN8rmbrNW0-Z"
      },
      "source": [
        "## The actual training loop!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nHktVH98kp4B",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "def train_loop():\n",
        "    epochs: int = 1000000  # Train until the cloud disconnects or the model stops improving\n",
        "    per_epoch_train_loss: List[float] = []\n",
        "    per_epoch_val_loss: List[float] = []\n",
        "    print(f\"number of train batches per epoch: {len(list_train_set)}\")\n",
        "    last_save_time = time.time()\n",
        "    global epoch: int = 0\n",
        "    global batch_num: int = 0\n",
        "    global val_step_calles: int = 0\n",
        "    global train_step_calles: int = 0\n",
        "    for epoch in range(epochs):\n",
        "        print(f\"epoch number: {epoch}\")\n",
        "        per_batch_train_loss: List[float] = []\n",
        "        per_batch_val_loss: List[float] = []\n",
        "        for batch_num in tqdm.tqdm(range(len(list_train_set))):  # tqdm is a progress bar\n",
        "            train_loss: tf.Tensor = train(list_train_set[batch_num])\n",
        "            float_val_loss = tf.keras.backend.eval(train_loss).item()\n",
        "            per_batch_train_loss.append(train_loss)\n",
        "            if batch_num % 8 == 0:  # 8 is number of training batches/number of val batches\n",
        "                # because training set is 80% of the data and val set is 10%\n",
        "                next_val_batch: tf.Tensor = list_val_set[batch_num // 8]\n",
        "                val_loss: tf.Tensor = validate(next_val_batch)\n",
        "                float_val_loss = tf.keras.backend.eval(val_loss).item()\n",
        "                per_batch_val_loss.append(val_loss)\n",
        "                wandb.log({\"epoch\": epoch, \"batch\": batch_num, \"per batch train loss\": train_loss, \n",
        "                        \"per batch val loss\": val_loss})\n",
        "                if time.time() - last_save_time > 3600.0 and val_loss < math.log(vocab_size):  \n",
        "                    # If the last save is more than a hour (3600 sec) ago\n",
        "                    # and if the predictions are not random\n",
        "                    check_point(check_points_path, model, per_epoch_val_loss[-1], per_epoch_train_loss[-1])\n",
        "                    last_save_time = time.time()\n",
        "                elif train_loss < 0.01:\n",
        "                    title: str = \"Over fitting or data leak\"\n",
        "                    message = f\"Training loss is {train_loss} and val loss is {val_loss} in the latest batch\"\n",
        "                    wandb.alert(title=title, text=message)\n",
        "                    print(title)\n",
        "                    print(message)\n",
        "                    return train_loss, val_loss\n",
        "                elif time.time() - last_save_time > 1800.0 and val_loss >= math.log(vocab_size):\n",
        "                    # if the prob of every token is 1/vocab_size, the loss is\n",
        "                    # -ln(1/vocab_size) = ln(vocab_size) \n",
        "                    # by the logrithem rule log(a^x)=xlog(a) where x = -1\n",
        "                    # if after 30 mins of training, the model predictions are still random\n",
        "                    title: str = \"Under fitting\"\n",
        "                    message = f\"training loss is {train_loss} and val loss is {val_loss} in the latest batch\"\n",
        "                    wandb.alert(title=title, text=message)\n",
        "                    print(title)\n",
        "                    print(message)\n",
        "                    return train_loss, val_loss\n",
        "        per_epoch_train_loss.append(statistics.mean(per_batch_train_loss))\n",
        "        per_epoch_val_loss.append(statistics.mean(per_batch_val_loss))\n",
        "        print(f\"train_loss: {per_epoch_train_loss[-1]}\")\n",
        "        print(f\"prob of right ans train: {loss_to_prob(per_epoch_train_loss[-1])}\")\n",
        "        print(f\"val_loss: {per_epoch_val_loss[-1]}\")\n",
        "        print(f\"prob of right ans val: {loss_to_prob(per_epoch_val_loss[-1])}\")\n",
        "        if len(per_epoch_val_loss) > 1:\n",
        "            if per_epoch_val_loss[-1] >= per_epoch_val_loss[-2]:\n",
        "                print(\"Validation loss increased. Stopped training\")\n",
        "                return per_epoch_train_loss[-1], per_epoch_val_loss[-1]\n",
        "        check_point(check_points_path, model, per_epoch_val_loss[-1], per_epoch_train_loss[-1])\n",
        "        last_save_time = time.time()\n",
        "        print(\"Saved checkpoint\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g5MukK3ynG7v",
        "outputId": "dca87c06-0140-41d9-c5bd-1e71ea9c9289"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "number of train batches per epoch: 8401\n",
            "epoch number: 0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/8401 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:AutoGraph could not transform <function train at 0x7f5fd4c5c290> and will run it as-is.\n",
            "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
            "Cause: name 'fscope' is not defined\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
            "WARNING: AutoGraph could not transform <function train at 0x7f5fd4c5c290> and will run it as-is.\n",
            "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
            "Cause: name 'fscope' is not defined\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
          ]
        }
      ],
      "source": [
        "with strategy.scope():\n",
        "    train_loss, val_loss = train_loop()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i48dY189QR1i"
      },
      "source": [
        "reproduce error"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V93HsJW-Vth-"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "resolver = tf.distribute.cluster_resolver.TPUClusterResolver();\n",
        "tf.config.experimental_connect_to_cluster(resolver);\n",
        "tf.tpu.experimental.initialize_tpu_system(resolver);\n",
        "strategy = tf.distribute.TPUStrategy(resolver);"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6EondR_UQUe4"
      },
      "outputs": [],
      "source": [
        "with strategy.scope():\n",
        "    exm_model = tf.keras.Sequential([\n",
        "        tf.keras.layers.Dense(2, activation=\"relu\", name=\"layer1\"),\n",
        "        tf.keras.layers.Dense(3, activation=\"relu\", name=\"layer2\"),\n",
        "        tf.keras.layers.Dense(1, name=\"layer3\"),\n",
        "    ])\n",
        "    exm_optimizer = tf.keras.optimizers.Adam()\n",
        "    mse = tf.keras.losses.MeanSquaredError()\n",
        "\n",
        "@tf.function(input_signature=(tf.TensorSpec(shape=[3,3], dtype=tf.int32),\n",
        "                              tf.TensorSpec(shape=[3], dtype=tf.int32)))\n",
        "def exm_train_step(inp: tf.Tensor, outp: tf.Tensor) -> tf.Tensor:\n",
        "    with tf.GradientTape() as tape:\n",
        "        pred: tf.Tensor = exm_model(inp) \n",
        "        loss_val: tf.Tensor = mse(outp, pred)\n",
        "    grads: tf.RaggedTensor = tape.gradient(loss_val, model.trainable_weights)\n",
        "    exm_optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
        "    return tf.math.reduce_mean(loss_val)\n",
        "\n",
        "\n",
        "exm_inp = tf.random.uniform(shape=[3, 3], dtype=tf.int32, maxval=100, minval=0)\n",
        "exm_out = tf.random.uniform(shape=[3], dtype=tf.int32, maxval=100, minval=0)\n",
        "\n",
        "\n",
        "exm_train_step(exm_inp, exm_out)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z-iopedNNjXs"
      },
      "source": [
        "## After training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1VtC8sP_NjXu"
      },
      "outputs": [],
      "source": [
        "test_loss = statistics.mean([validate(test_batch) for test_batch in tqdm.tqdm(list_test_set)])\n",
        "print(f\"Test loss: {test_loss}\")\n",
        "check_point(check_points_path, model, val_loss, train_loss, test_loss)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "collapsed_sections": [],
      "name": "create_models.ipynb",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernel_info": {
      "name": "python3"
    },
    "kernelspec": {
      "display_name": "Python 3.9.12 ('base')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    },
    "vscode": {
      "interpreter": {
        "hash": "4c667eb41e29edd16195f2712b5916a29b36d4e6f17fe7c0a7edbb7a73b8ba8a"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
